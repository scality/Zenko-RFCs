
# ZIP#1 (Zenko Improvement Proposal #1) - Workflow Engine

## Overview

### Problem Description

### Use-cases Description

What uses-cases does this address? what impact on actors does this change have ?

Ensure you are clear about the actors in each use case: developers, users, deployers, etc

#### Notification (SNS) API
We may support the PutBucketNotification API and have the option to trigger a workflow in case of an event.

#### “Authorization” (SAS) API
We may support an API similar to SNS but which can call a workflow before actual any data to be sent to CloudServer (e.g. avoiding that 1TB files are sent if unauthorized).

Sub use cases:
##### Restraint Access by IP Address
Allows unauthenticated access by IP address.
##### Refuse PUTs based on Time
E.g. for financial needs it is forbidden to update the system after 5pm.
##### Refuse PUTs based on Size of files
In this mode we may enforce the 100-continue mechanism.
##### Bucket Object Prefix
Restricts users by prefixes.

#### Cron-Based “Search” Workflows
These workflows are triggered by a crontab and typically generate their initial queues by searching objects in locations, e.g. aws:foo/*.jpg.


### Proposed Changes

Here is how we propose to solve the problem.

Scope of the effort.


#### Architecture Diagram

```

+-----------------+
|                 |
|                 |
|                 | Graphical Workflow Edition
|   Orbit UI      |
|                 | Statistics, Dashboard
+---------------+-+
                |
                |
                |              +-------------------------+
                |              |                         |
          JSON workflow        |                         |
                |              |   Workflow Engine       |
                |              |                         |
             +--v--------+     |                         |
             |Management +----->                         |
             |Agent      |     +-^----------------^------+
             +-----------+       |                |
                                 |                |
                                 |                |
                                 |                |
                  +------------+-+                |
                  |            |             +----+------+
                  |            |             |           |
                  | plugin1    |             |           |
                  |            |             | plugin2   |
                  +------------+             |           |
                                             +-----------+

```

#### Components

| Component | Description |
| ------------- | ------------- |
| Graphical workflow edition  | This component is done in Orbit UI. It is possible to define multiple workflows. When saving the workflows, it generates a JSON file that describe the set of workflows that can be read by the Workflow Manager. |
| Workflow Engine  | This component is implemented in Backbeat and executes the actual JSON defined workflows  |
| Plugins | Plugins register to the workflow engine and perform single tasks in the workflow, e.g. compression, trigger lambda. |
| Workflow Statistics | The workflow manager maintains and reports statistics about objects traversing each single workflow and each single step of each workflow. |
| Workflow Dashboard | At any time the system administrator can view graphically the statistics on Orbit UI. |
| Management Agent | Agent that communicates with with Orbit |

#### Requirements


##### Execution of the workflow
Orbit sends the new JSON file to the Workflow Manager which will be responsible for executing it. 
Every step will try to match an object name or some properties and possibly trigger an action which can be a built-in function or an external module (e.g. encryption, compression, data movement, etc).

##### Multiple parallel workflows
The Workflow Manager can actually executes many workflows

#####  A workflow has many steps
A workflow is composed of many steps, having one or many inputs.

##### Source of a workflow
At every beginning of a workflow there is an MD source (events generated by actual traffic or by a cloud trace (e.g. Admin API or other cloud specific event logs like AWS Lambda or Azure functions). The input of a workflow can also be the result of MD searches executing at specific times (e.g. searchObject(“bar”, “*.jpg” ) bound to a crontab). 

##### Terminator of a workflow
At every end of a workflow there is a terminator that stops the workflow. The terminator can be e.g. a copy on a cloud  - putObjectToLocation(), or a trigger of a Lambda function - executeLambda().

##### Tolerance of impedance between plugins
Since all plugins don’t process information at the same time, and for failure resilience, every time a workflow step needs to execute a specific plugin we perform input AND output queuing (e.g. in a specific topic), the output being the input of the next plugin and so on.

##### Multi-language support
A Workflow module could be written in any language (nodejs, python, go, C, C++, etc) so we will favor a REST API for interacting between modules.

##### Unity of language for the framework itself
The workflow manager is a Backbeat task written in NodeJS.

##### Type of workflows
Workflow management could occur at different steps of Cloudserver:
Authorization step
Notification step (event)
Cron based, e.g. search

##### Nature of queues
Workflows queues only contain some metadata: the object name the object ID and the “location” where to find the object (if possible in a transient source).

##### Type of plugins
Some plugins are MD only (intput/output), some are DATA+MD (input/output), some are hybrid MD input/DATA+MD output, or DATA+MD input/MD output

##### Data manipulation in plugins
Plugins that manipulate data read the data from Cloudserver with the location:objectID specified in queues metadata. The output (if any) of the plugin is streamed to an object in CloudServer and the new location:objectID is stored in the queue for the next plugin in line.

#### Generic Workflow Functions
Here is a non-exhaustive list of workflow functions:

##### filterByIPAddress
Filter an S3 operation by IP address

##### putObjectToLocation(location)
Put a given object to a destination location.

##### newObjectEvent(location)
Triggered when a new object is created on a given location. This plugin will require to be fed with events coming from a location.

##### encryptObject()
Encrypt an object

##### compressObject()
Compress an object

##### executeLambda()
Execute a code snippet, for e.g bucket notification

##### searchObjects(location, regexp)
Such all objects in a given location that match regexp. Can be used as a MD source.

##### erasureCode(n, k)
Generate an n data and k coding fragments (as new objects), to be dispatched on different locations.

##### tagObject()
Tag the specific object with an attribute

Examples of 3rd-party plugins:

##### encodeVideo(format)
Encode a video in the given format, generates a new object.


### Alternatives

What are other ways we could implement this, and why are we not using these.

