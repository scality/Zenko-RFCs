
# ZIP#1 (Zenko Improvement Proposal #1) - Workflow Engine

## Overview

### Problem Description

### Use-cases Description

What uses-cases does this address? what impact on actors does this change have ?

Ensure you are clear about the actors in each use case: developers, users, deployers, etc

#### Notification (SNS) API
We may support the PutBucketNotification API and have the option to trigger a workflow in case of an event.

#### “Authorization” (SAS) API
We may support an API similar to SNS but which can call a workflow before actual any data to be sent to CloudServer (e.g. avoiding that 1TB files are sent if unauthorized).

Sub use cases:
##### Restraint Access by IP Address
Allows unauthenticated access by IP address.
##### Refuse PUTs based on Time
E.g. for financial needs it is forbidden to update the system after 5pm.
##### Refuse PUTs based on Size of files
In this mode we may enforce the 100-continue mechanism.
##### Refuse PUTs based on presence of some attributes
In this mode we may enforce some attributes in the PUT headers.
##### Bucket Object Prefix
Restricts users by prefixes.

#### Cron-Based “Search” Workflows
These workflows are triggered by a crontab and typically generate their initial queues by searching objects in locations, e.g. aws:foo/*.jpg.


### Proposed Changes

Here is how we propose to solve the problem.

Scope of the effort.


#### Architecture Diagram

```

+-----------------+
|                 |
|                 |
|    Orbit UI     | Graphical Workflow Edition
|                 |
|                 | Statistics, Dashboard
+---------------+-+
                |
                |
                |              +-------------------------+
                +              |                         |
          JSON workflow        |                         |
                +              |   Workflow Engine       |
                |              |                         |
             +--v--------+     |                         |
             |Management +----->                         |
             |Agent      |     +-^----------------^------+
             +-----------+       |                |
                                 |                |
                                 |                |
                                 |                |
+----------------+            +--+----------  +---+-------+
|                |   DATA     |            |  |           |
|  Cloud Server  |            |            |  |           |
|                <------------+ plugin1    |  | plugin2   |
|                |            |            |  |           |
+----------------+            +------------+  +-----------+


```

#### Components

| Component | Description |
| ------------- | ------------- |
| Graphical workflow edition  | This component is done in Orbit UI. It is possible to define multiple workflows. When saving the workflows, it generates a JSON file that describe the set of workflows that can be read by the Workflow Manager. |
| Workflow Engine  | This component is implemented in Backbeat and executes the actual JSON defined workflows  |
| Plugins | Plugins register to the workflow engine and perform single tasks in the workflow, e.g. compression, trigger lambda. |
| Workflow Statistics | The workflow manager maintains and reports statistics about objects traversing each single workflow and each single step of each workflow. |
| Workflow Dashboard | At any time the system administrator can view graphically the statistics on Orbit UI. |
| Management Agent | Agent that communicates with with Orbit |
| Cloud Server | Needed when a plugin requires reading or writing data |

#### Requirements


##### Execution of the workflow
Orbit sends the new JSON file to the Workflow Manager which will be responsible for executing it. 
Every step will try to match an object name or some properties and possibly trigger an action which can be a built-in function or an external module (e.g. encryption, compression, data movement, etc).

##### Non-disruption of existing traffic
It is not possible to modify a workflow in place because it would be too disruptive for the current traffic: a new workflow is always created. Instead, it is proposed to apply the new workflow to a percentage of the traffic (e.g. 5%).

##### Multiple parallel workflows
The Workflow Manager can actually executes many workflows

#####  A workflow has many steps
A workflow is composed of many steps, having one or many inputs.

##### Source of a workflow
At every beginning of a workflow there is an MD source (events generated by actual traffic or by a cloud trace (e.g. Admin API or other cloud specific event logs like AWS Lambda or Azure functions). The input of a workflow can also be the result of MD searches executing at specific times (e.g. searchObject(“bar”, “*.jpg” ) bound to a crontab). 

##### Terminator of a workflow
At every end of a workflow there is a terminator that stops the workflow. The terminator can be e.g. a copy on a cloud  - putObjectToLocation(), or a trigger of a Lambda function - executeLambda().

##### Tolerance of impedance between plugins
Since all plugins don’t process information at the same time, and for failure resilience, every time a workflow step needs to execute a specific plugin we perform input AND output queuing (e.g. in a specific topic), the output being the input of the next plugin and so on.

##### Multi-language support
A Workflow module could be written in any language (nodejs, python, go, C, C++, etc) so we will favor a REST API for interacting between modules.

##### Unity of language for the framework itself
The workflow manager is a Backbeat task written in NodeJS.

##### Type of workflows
Workflow management could occur at different steps of Cloudserver:
Authorization step
Notification step (event)
Cron based, e.g. search

##### Nature of queues
Workflows queues only contain some metadata: the object name the object ID and the “location” where to find the object (if possible in a transient source).

##### Type of plugins
Some plugins are MD only (intput/output), some are DATA+MD (input/output), some are hybrid MD input/DATA+MD output, or DATA+MD input/MD output

##### Data manipulation in plugins
Plugins that manipulate data read the data from Cloudserver with the location:objectID specified in queues metadata. The output (if any) of the plugin is streamed to an object in CloudServer and the new location:objectID is stored in the queue for the next plugin in line.

#### Generic Workflow Functions
Here is a non-exhaustive list of workflow functions:

##### filterByIPAddress()
Filter an S3 operation by IP address

##### putObjectToLocation(location)
Put a given object to a destination location.

##### newObjectEvent(location)
Triggered when a new object is created on a given location. This plugin will require to be fed with events coming from a location.

##### encryptObject()
Encrypt an object

##### compressObject()
Compress an object

##### executeLambda()
Execute a code snippet, for e.g bucket notification

##### searchObjects(location, regexp)
Such all objects in a given location that match regexp. Can be used as a MD source.

##### erasureCode(n, k)
Generate an n data and k coding fragments (as new objects), to be dispatched on different locations.

##### tagObject()
Tag the specific object with an attribute

Examples of 3rd-party plugins:

##### encodeVideo(format)
Encode a video in the given format, generates a new object.

#### Example of Execution

| Step | Description |
| ------------- | ------------- |
| 1 - Plugin1 registers | The first plugin registers itself and its tuning parameters to the workflow engine |
| 2 - Plugin2 registers | The second plugin registers itself and its tuning parameters to the workflow engine |
| 3 - Workflow engine to register plugins | The workflow engine registers the new plugins and their tuning parameters to the configuration hub (Management agent). |
| 4 - Management agent to register plugins | The Management agent transmits the new plugins and their tuning parameters to Orbit |
| 5 - Edit workflow | An account edits the workflow in Orbit with a GUI. Since the 2 plugins are registered, the account can drag-and-drop the 2 plugins and tune their parameters.|
| 6 - Push JSON workflow| When the account clicks on “deploy” the new workflow is sent to the Management Agent.|
| 7- Store config | The workflow is stored in the configuration |
| 8 - Read config | The workflow engine producer (WEP) reads the config |
| 9 - Execute workflow | The WEP executes the workflow.|
| 10 - Plugin1 queue | The first step of the workflow is e.g. to feed the plugin queue with events coming from a specific location, if the plugins requires it. It is possible that the plugins only requires to be launched at a specific time (crontab) and then the WMP will only manage the output queue.|
| 11 - Produce plugin1 | The events for the first plugin (e.g. MD of objects) are queued in the dedicated plugin1 queue.|
| 12 - Consume plugin1 | The Workflow engine consumer (WEC) reads the events for the first plugin. |
| 13 - Call plugin1 | For each event in the queue the WEC call the plugin1 with a REST interface. By defaults only the MD of the objects are sent along, if the plugin needs data it can use the location:objectId and ask CloudServer|
| 14 - Output plugin1 | For each input event there is an output event which is returned to the WEC. |
| 15 - Plugin2 queue | The WEC (acting as a WEP) feeds the second plugin input queue. |
| 16 - Produce plugin2 | The MD are queued for plugin2 consumption.|
| 17 - Consume plugin2 | The WEC consumes the queue.|
| 18 - Call plugin2 | Plugin2 is called on a REST interface. Assuming plugin2 is a terminator plugin, the workflow ends here.|

### Alternatives

What are other ways we could implement this, and why are we not using these.

