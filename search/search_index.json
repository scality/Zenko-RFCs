{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Citadel Repository of requirements, design documents for Scality S3 and Zenko projects","title":"Citadel"},{"location":"#citadel","text":"Repository of requirements, design documents for Scality S3 and Zenko projects","title":"Citadel"},{"location":"design/","text":"Design","title":"General"},{"location":"design/#design","text":"","title":"Design"},{"location":"design/ballot-launcher/","text":"Ballot Launcher Overview This specification presents the design for ballot : a coordinated task-launching utility. Problem The S3C stack currently lacks cluster-wide coordination of maintenance tasks and singleton services. This creates issues such as: Single points of failure - if the server singled out to run a task (either randomly or because it's the first of a list) becomes unavailable, then the task is not run as expected. Risk of concurrent execution of tasks that may interfere with each other. Processing load may be uneven. Knowing what will run where (and what has run where) requires looking at logs and configuration. Use Cases Tasks The following task types will benefit from coordination: Scheduled tasks (cron jobs), either on a timetable or an interval between expected runs. Always-on services or long-running one-shot commands that should run only one copy for data consistency or resource allocation reasons. Use Case for Developers As a developer integrating a new processing job that fits the Impacted Tasks definition above, I need a way for the deployment tool (Federation) to schedule running the script on all eligible servers without any special treatment at install time, with a guarantee that it will not run on multiple servers at the same time, but will still run no matter which servers are down. Use Case for Operations As an operations person, I need to be sure that maintenance jobs: Run when expected regardless of system state. Can be easily inspected. Can be paused if system state requires it. Technical Details Principle A launcher, ballot is introduced and used to run commands. Before executing the provided command, ballot tries to aquire leadership on a ZooKeeper path specific to the command (using the standard leader election recipe). The leader then runs the command, while followers wait. When the leader resigns, the next follower in line takes over leadership and runs the command. Proposed UX To give a clear idea of expected capabilities, the envisioned user experience is described below. Run a Simple Task ballot --zookeeper-path /com/scality/tasks/mycluster/consolidate \\ run once --candidate-id dc01-room01-rack01-server01 -- python consolidate.py Run a Scheduled Task ballot --zookeeper-path /com/scality/tasks/mycluster/consolidate \\ run cron --schedule @hourly --candidate-id dc01-room01-rack01-server01 -- python consolidate.py Show Information on a Task Display information on a distributed task. Useful information that may be displayed includes: current leader's candidate ID, list of follower's candidate IDs, date and time of election, hostname, PID. ballot --zookeeper-path /com/scality/tasks/mycluster/consolidate info Pause Execution of a Task To conserve resources during an outage or planned maintenance (just disabling future elections, not impacting currently running jobs): ballot --zookeeper-path /com/scality/tasks/mycluster/consolidate pause To terminate currently running jobs: ballot --zookeeper-path /com/scality/tasks/mycluster/consolidate pause --terminate Resume Execution of a Task To resume a paused job: ballot --zookeeper-path /com/scality/tasks/mycluster/consolidate resume List All Known Tasks To show a cluster-wide list of tasks and their status (info such as which node is the leader, a list of followers, election time, hostname, PID). ballot --zookeeper-path /com/scality/tasks/mycluster status Integration points ballot can be used as a direct ENTRYPOINT in Docker images, as a command-line prefix for containers, or as a trampoline in supervisord configuration files, whichever is least intrusive. Changes Introduced Where tasks were assigned to one server at deployment time, they instead are configured to run on all eligible servers, and use ballot as a launcher, which ensures uniqueness and fallback. Interaction with Other Components Ballot requires a functioning ZooKeeper ensemble to operate. Given its limited responsibilities, other components/tools are required for general operation, such as: /usr/bin/env for setting environment variables (e.g. ballot run once -- env VAR=value python script.py ). Log shipping/aggregation as configured in S3C - ballot will pass through the task's logs. An always-up process manager (Docker, supervisord) to handle log redirection to a known destination, restart and backoff decisions for the ballot process tree. Monitoring Monitoring a command run with ballot should be the same as monitoring the same command run without ballot; no additional burden should be added. To achieve this, the commands's logs and exit code can be passed through ballot . Launcher-specific issues can be interpreted as a task failure. Monitoring of the task itself should continue as usual, and monitoring of ZooKeeper becomes more crucial as more workloads depend on it being available. For later consideration: ballot could include some options to integrate with existing alerting mechanisms. Failure Scenarios Leader Election Failure If leader election fails for any reason (ZooKeeper down or unreachable, bad configuration, ...), the following resolutions can be chosen from: retry : Retries the election process until a leader is successfully elected. exit : Exits the ballot process and lets the underlying process manager ( docker , supervisord , ...) apply its restart and backoff policy. run-anyway : Ignores the failure and runs the command anyway. This is useful in case it's more important to have the command run, even if it is run multiple times in parallel, than for it to not run at all. Task Failure Tasks can fail for reasons such as unhealthy host, network issue/partition, application bug w.r.t. bad data handling, etc. No unique solution would work, so a range of resolutions can be chosen from: reelect : Drops leadership so that a new leader election can take place, potentially allowing a more healthy host to take over. rerun : Runs the command again immediately, and repeats until the command succeeds. exit : Exits the ballot process and lets the underlying process manager Docker, supervisord, ...) apply its restart and backoff policy. ignore : Ignores the failure and exits with a success code anyway. In cron mode, ignores the failure and waits for the next scheduled trigger. Note: failures to execute the command (like a missing binary) are handled by the same mechanism as failure of a command that was exec 'd successfully. In this case, a failure policy of reelect would make sense while rerun would not be of much help. Note: for flexibility, the same actions can be performed on successful completion of the child process. ZooKeeper Failure after Leader Election If ZooKeeper loses quorum, crashes, or becomes unreachable, ballot will try to reconnect in the background, and in the meantime not take any action: The leader still considers itself the leader, and carries on with waiting for its child process completion. In cron mode, it continues scheduling. Any followers will continuously try to become leaders (or be restarted) according to the Leader Election Failure policy described above, and should not succeed. Upon reconnection to ZooKeeper, ballot ZooKeeper session IDs are matched to the owners of the ephemeral proposal znodes, picking up the leader/follower roles as they were before the disruption. If any ballot process was started after the disruption, then it will automatically join as a follower, as long as some ballot processes with longer-running session IDs are present. In the special case where ballot processes are configured to exit or reelect and are not run in cron mode, once all commands are done running, nothing will run until ZooKeeper is back online, which can be seen as the worst-case scenario. Alternatives Other Technologies Tools such as lockrun are often used by sysadmins and provide the solution to the \"at most once\" problem on a single server/container, but still require coordination so that no single server is considered special and can still be taken out of service without impacting task availability. Kubernetes provides resources that solve these problems such as jobs , cronjobs , statefulsets with one replica. However S3C does not run on Kubernetes. Dkron comes with its own Raft quorum, which adds deployment and operation burden. Its free/commercial split license can be inconvenient. Chronos is very complete and comes with good reporting capabilities. It runs on top of Mesos, which makes it heavy and impractical in the context of S3C. Other Approaches Leader Election Functionality Since the primary goal was to be as unintrusive and scalable (developer-time wise) as possible, implementing a leader election in each task type was not a viable option. Cron Functionality To keep things simple and focused, one approach considered was to not include the cron functionality in ballot , and instead either: Have each targeted task implement the cron functionality itself. Ruled out because of code reusability concerns, and lots of boilerplate involved in the leader election. Run ballot run my-command in a crontab line. This is prone to overflowing ZooKeeper if executions were to stack up waiting for their turn. Implementing invocation tracking would also be required to make sure that once a scheduled invocation finishes successfully, no other should run for the same trigger. Run ballot run anacron -t my-crontab-file . This is close to a workable solution, but increases the work needed for integration as it does not allow \"smart\" retry policies, and pass-through of exit codes and logs.","title":"Ballot Launcher"},{"location":"design/ballot-launcher/#ballot-launcher","text":"","title":"Ballot Launcher"},{"location":"design/ballot-launcher/#overview","text":"This specification presents the design for ballot : a coordinated task-launching utility.","title":"Overview"},{"location":"design/ballot-launcher/#problem","text":"The S3C stack currently lacks cluster-wide coordination of maintenance tasks and singleton services. This creates issues such as: Single points of failure - if the server singled out to run a task (either randomly or because it's the first of a list) becomes unavailable, then the task is not run as expected. Risk of concurrent execution of tasks that may interfere with each other. Processing load may be uneven. Knowing what will run where (and what has run where) requires looking at logs and configuration.","title":"Problem"},{"location":"design/ballot-launcher/#use-cases","text":"","title":"Use Cases"},{"location":"design/ballot-launcher/#tasks","text":"The following task types will benefit from coordination: Scheduled tasks (cron jobs), either on a timetable or an interval between expected runs. Always-on services or long-running one-shot commands that should run only one copy for data consistency or resource allocation reasons.","title":"Tasks"},{"location":"design/ballot-launcher/#use-case-for-developers","text":"As a developer integrating a new processing job that fits the Impacted Tasks definition above, I need a way for the deployment tool (Federation) to schedule running the script on all eligible servers without any special treatment at install time, with a guarantee that it will not run on multiple servers at the same time, but will still run no matter which servers are down.","title":"Use Case for Developers"},{"location":"design/ballot-launcher/#use-case-for-operations","text":"As an operations person, I need to be sure that maintenance jobs: Run when expected regardless of system state. Can be easily inspected. Can be paused if system state requires it.","title":"Use Case for Operations"},{"location":"design/ballot-launcher/#technical-details","text":"","title":"Technical Details"},{"location":"design/ballot-launcher/#principle","text":"A launcher, ballot is introduced and used to run commands. Before executing the provided command, ballot tries to aquire leadership on a ZooKeeper path specific to the command (using the standard leader election recipe). The leader then runs the command, while followers wait. When the leader resigns, the next follower in line takes over leadership and runs the command.","title":"Principle"},{"location":"design/ballot-launcher/#proposed-ux","text":"To give a clear idea of expected capabilities, the envisioned user experience is described below.","title":"Proposed UX"},{"location":"design/ballot-launcher/#run-a-simple-task","text":"ballot --zookeeper-path /com/scality/tasks/mycluster/consolidate \\ run once --candidate-id dc01-room01-rack01-server01 -- python consolidate.py","title":"Run a Simple Task"},{"location":"design/ballot-launcher/#run-a-scheduled-task","text":"ballot --zookeeper-path /com/scality/tasks/mycluster/consolidate \\ run cron --schedule @hourly --candidate-id dc01-room01-rack01-server01 -- python consolidate.py","title":"Run a Scheduled Task"},{"location":"design/ballot-launcher/#show-information-on-a-task","text":"Display information on a distributed task. Useful information that may be displayed includes: current leader's candidate ID, list of follower's candidate IDs, date and time of election, hostname, PID. ballot --zookeeper-path /com/scality/tasks/mycluster/consolidate info","title":"Show Information on a Task"},{"location":"design/ballot-launcher/#pause-execution-of-a-task","text":"To conserve resources during an outage or planned maintenance (just disabling future elections, not impacting currently running jobs): ballot --zookeeper-path /com/scality/tasks/mycluster/consolidate pause To terminate currently running jobs: ballot --zookeeper-path /com/scality/tasks/mycluster/consolidate pause --terminate","title":"Pause Execution of a Task"},{"location":"design/ballot-launcher/#resume-execution-of-a-task","text":"To resume a paused job: ballot --zookeeper-path /com/scality/tasks/mycluster/consolidate resume","title":"Resume Execution of a Task"},{"location":"design/ballot-launcher/#list-all-known-tasks","text":"To show a cluster-wide list of tasks and their status (info such as which node is the leader, a list of followers, election time, hostname, PID). ballot --zookeeper-path /com/scality/tasks/mycluster status","title":"List All Known Tasks"},{"location":"design/ballot-launcher/#integration-points","text":"ballot can be used as a direct ENTRYPOINT in Docker images, as a command-line prefix for containers, or as a trampoline in supervisord configuration files, whichever is least intrusive.","title":"Integration points"},{"location":"design/ballot-launcher/#changes-introduced","text":"Where tasks were assigned to one server at deployment time, they instead are configured to run on all eligible servers, and use ballot as a launcher, which ensures uniqueness and fallback.","title":"Changes Introduced"},{"location":"design/ballot-launcher/#interaction-with-other-components","text":"Ballot requires a functioning ZooKeeper ensemble to operate. Given its limited responsibilities, other components/tools are required for general operation, such as: /usr/bin/env for setting environment variables (e.g. ballot run once -- env VAR=value python script.py ). Log shipping/aggregation as configured in S3C - ballot will pass through the task's logs. An always-up process manager (Docker, supervisord) to handle log redirection to a known destination, restart and backoff decisions for the ballot process tree.","title":"Interaction with Other Components"},{"location":"design/ballot-launcher/#monitoring","text":"Monitoring a command run with ballot should be the same as monitoring the same command run without ballot; no additional burden should be added. To achieve this, the commands's logs and exit code can be passed through ballot . Launcher-specific issues can be interpreted as a task failure. Monitoring of the task itself should continue as usual, and monitoring of ZooKeeper becomes more crucial as more workloads depend on it being available. For later consideration: ballot could include some options to integrate with existing alerting mechanisms.","title":"Monitoring"},{"location":"design/ballot-launcher/#failure-scenarios","text":"","title":"Failure Scenarios"},{"location":"design/ballot-launcher/#leader-election-failure","text":"If leader election fails for any reason (ZooKeeper down or unreachable, bad configuration, ...), the following resolutions can be chosen from: retry : Retries the election process until a leader is successfully elected. exit : Exits the ballot process and lets the underlying process manager ( docker , supervisord , ...) apply its restart and backoff policy. run-anyway : Ignores the failure and runs the command anyway. This is useful in case it's more important to have the command run, even if it is run multiple times in parallel, than for it to not run at all.","title":"Leader Election Failure"},{"location":"design/ballot-launcher/#task-failure","text":"Tasks can fail for reasons such as unhealthy host, network issue/partition, application bug w.r.t. bad data handling, etc. No unique solution would work, so a range of resolutions can be chosen from: reelect : Drops leadership so that a new leader election can take place, potentially allowing a more healthy host to take over. rerun : Runs the command again immediately, and repeats until the command succeeds. exit : Exits the ballot process and lets the underlying process manager Docker, supervisord, ...) apply its restart and backoff policy. ignore : Ignores the failure and exits with a success code anyway. In cron mode, ignores the failure and waits for the next scheduled trigger. Note: failures to execute the command (like a missing binary) are handled by the same mechanism as failure of a command that was exec 'd successfully. In this case, a failure policy of reelect would make sense while rerun would not be of much help. Note: for flexibility, the same actions can be performed on successful completion of the child process.","title":"Task Failure"},{"location":"design/ballot-launcher/#zookeeper-failure-after-leader-election","text":"If ZooKeeper loses quorum, crashes, or becomes unreachable, ballot will try to reconnect in the background, and in the meantime not take any action: The leader still considers itself the leader, and carries on with waiting for its child process completion. In cron mode, it continues scheduling. Any followers will continuously try to become leaders (or be restarted) according to the Leader Election Failure policy described above, and should not succeed. Upon reconnection to ZooKeeper, ballot ZooKeeper session IDs are matched to the owners of the ephemeral proposal znodes, picking up the leader/follower roles as they were before the disruption. If any ballot process was started after the disruption, then it will automatically join as a follower, as long as some ballot processes with longer-running session IDs are present. In the special case where ballot processes are configured to exit or reelect and are not run in cron mode, once all commands are done running, nothing will run until ZooKeeper is back online, which can be seen as the worst-case scenario.","title":"ZooKeeper Failure after Leader Election"},{"location":"design/ballot-launcher/#alternatives","text":"","title":"Alternatives"},{"location":"design/ballot-launcher/#other-technologies","text":"Tools such as lockrun are often used by sysadmins and provide the solution to the \"at most once\" problem on a single server/container, but still require coordination so that no single server is considered special and can still be taken out of service without impacting task availability. Kubernetes provides resources that solve these problems such as jobs , cronjobs , statefulsets with one replica. However S3C does not run on Kubernetes. Dkron comes with its own Raft quorum, which adds deployment and operation burden. Its free/commercial split license can be inconvenient. Chronos is very complete and comes with good reporting capabilities. It runs on top of Mesos, which makes it heavy and impractical in the context of S3C.","title":"Other Technologies"},{"location":"design/ballot-launcher/#other-approaches","text":"","title":"Other Approaches"},{"location":"design/ballot-launcher/#leader-election-functionality","text":"Since the primary goal was to be as unintrusive and scalable (developer-time wise) as possible, implementing a leader election in each task type was not a viable option.","title":"Leader Election Functionality"},{"location":"design/ballot-launcher/#cron-functionality","text":"To keep things simple and focused, one approach considered was to not include the cron functionality in ballot , and instead either: Have each targeted task implement the cron functionality itself. Ruled out because of code reusability concerns, and lots of boilerplate involved in the leader election. Run ballot run my-command in a crontab line. This is prone to overflowing ZooKeeper if executions were to stack up waiting for their turn. Implementing invocation tracking would also be required to make sure that once a scheduled invocation finishes successfully, no other should run for the same trigger. Run ballot run anacron -t my-crontab-file . This is close to a workable solution, but increases the work needed for integration as it does not allow \"smart\" retry policies, and pass-through of exit codes and logs.","title":"Cron Functionality"},{"location":"design/bucket-encryption/","text":"Bucket Encryption Old support for bucket-level encryption create_encrypted_bucket.js The previous way of encrypting objects in buckets is by using a script to create the bucket like: docker exec scality-s3 node bin/create_encrypted_bucket.js -h 10.10.61.61 -p 8000 -a XIJG9OTSMLDDIEWX21HT -k BTNMVTDBujqpTUEVYVYtfhxExCWy3KvDlad4hmwC -b my-encrypted-bucket The script is creating a bucket through the REST API, passing an extra proprietary header \"x-amz-scal-server-side-encryption: AES256\" to enable encryption using a newly created bucket master key stored in the KMS server, and referenced from the bucket metadata. It's also possible with the existing REST API to specify an existing master key ID to use by providing the extra \"x-amz-scal-server-side-encryption-aws-kms-key-id\" header, although the create_encrypted_bucket.js script does not provide this option. We plan to keep the compatibility with those headers for now but the compatibility will be removed eventually in favor of the bucket encryption API. Bucket Encryption API We now support bucket encryption using the standard S3 APIs, to enable or disable encryption on existing buckets: PutBucketEncryption GetBucketEncryption DeleteBucketEncryption PutBucketEncryption We support the following rule to enable encryption automatically on all objects in the bucket: <ServerSideEncryptionConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"> <Rule> <ApplyServerSideEncryptionByDefault> <SSEAlgorithm>AES256</SSEAlgorithm> </ApplyServerSideEncryptionByDefault> </Rule> ... </ServerSideEncryptionConfiguration> Alternatively, we also allow specifying a master key ID to use for the bucket that will be sent when contacting the KMS via the KMSMasterKeyID tag, instead of letting S3C generate one automatically: <ServerSideEncryptionConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"> <Rule> <ApplyServerSideEncryptionByDefault> <SSEAlgorithm>aws:kms</SSEAlgorithm> <KMSMasterKeyID>abcdef</KMSMasterKeyID> </ApplyServerSideEncryptionByDefault> </Rule> ... </ServerSideEncryptionConfiguration> Note : only one <Rule> entry is allowed in the server-side encryption configuration. Note : the <BucketKeyEnabled> rule option is not supported. Implementation of PutBucketEncryption Regarding Bucket Metadata If the serverSideEncryption does not exist, we create it and fill with the relevant attributes (see Internal Bucket Metadata ) If the serverSideEncryption exists, we set its mandatory attribute to true , and possibly set the algorithm to the new configured algorithm, and if it is aws:kms , the KMSMasterKeyId also gets stored in the configuredMasterKeyId attribute. We do not change the other attributes, notably the masterKeyId is not replaced by a new one. GetBucketEncryption Returns the bucket encryption configuration, as specified by a previous PutBucketEncryption. Note : due to the bucket metadata format preservation, it should also be returning an existing bucket-level encryption configuration for buckets created with the legacy x-amz-scal-server-side-encryption header, that did not get an explicit configuration applied via PutBucketEncryption. Implementation of GetBucketEncryption Regarding Bucket Metadata See Internal Bucket Metadata The implementation reads the fields stored in the serverSideEncryption bucket metadata attribute, and returns the formatted XML response filled with the needed information. If there is no serverSideEncryption attribute, or if the serverSideEncryption attribute does not contain the mandatory: true field, return ServerSideEncryptionConfigurationNotFoundError . DeleteBucketEncryption Deletes the existing bucket encryption configuration, removing any default bucket encryption policy for the bucket hence storing new objects unencrypted unless the requests provide the header x-amz-server-side-encryption . Internally, some of the bucket encryption metadata will be kept but the mandatory flag is removed (see Internal Metadata section). Implementation of DeleteBucketEncryption Regarding Bucket Metadata See Internal Bucket Metadata When the bucket encryption configuration is deleted, we don't entirely delete the serverSideEncryption section, because we want to keep the existing parameters, notably the masterKeyId . Instead, we drop the mandatory field and configuredMasterKeyId (if set), and keep the other fields. Deleting a bucket encryption configuration that does not exist (i.e. no serverSideEncryption section or its mandatory field is unset) returns a silent success to the client. Application Logic for Object Encryption The following steps determine when and how to apply encryption to a particular object PUT request: Does the request contain the x-amz-server-side-encryption header? yes : encrypt object with the algorithm value of x-amz-server-side-encryption and master key ID x-amz-server-side-encryption-aws-kms-key-id if it is set and the algorithm value is aws:kms , or the bucket managed key under serverSideEncryption.masterKeyId otherwise no : continue Does the bucket have a default encryption configuration? (i.e the bucket metadata field serverSideEncryption.mandatory is true ) yes : encrypt object with the algorithm value of serverSideEncryption.algorithm and either the serverSideEncryption.configuredMasterKeyId if it is set, or the master key ID serverSideEncryption.masterKeyId otherwise no : do not encrypt the object The following steps give the procedure when an object is to be encrypted: if the algorithm value is AES256 : create server-side bucket encryption parameters if they do not exist yet in the serverSideEncryption bucket metadata field: set the algorithm attribute to AES256 and the masterKeyId to a new master key ID for that bucket, as well as cryptoScheme with value 1 . Do not set the mandatory field, as this configuration shall not apply automatically to new objects. encrypt the object with a new data key, itself encrypted with the bucket master key store the encrypted data key and master key ID (the same as the bucket master key ID) in the object metadata if the algorithm value is aws:kms and a master key ID is provided: encrypt the object with a new data key, itself encrypted with the user-specified master key store the encrypted data key and user-specified master key ID in the object metadata. Do not create or alter the bucket server-side encryption parameters. if the algorithm value is aws:kms but no master key ID is provided, follow the AES256 logic above. Internal Metadata The internal metadata format should not change with the introduction of encryption API support. Internal Bucket Metadata The internal bucket metadata related to encryption is stored under the serverSideEncryption attribute. It may be of three slightly different formats, described below. Note : The serverSideEncryption field used to be created automatically when the x-amz-scal-server-side-encryption was provided at bucket creation time. It is now created dynamically, either when a default bucket encryption configuration is applied to the bucket or when the first object requesting encryption (through the x-amz-server-side-encryption header) is stored in the bucket. Managed KMS Master Key Without a Default Configuration When a bucket has a managed encryption key associated to it, but no default encryption applied to new objects automatically, its metadata serverSideEncryption looks like this: serverSideEncryption: { cryptoScheme: 1, algorithm: \"AES256\", masterKeyId: \"48c5b43c9ba57f052deac9c081b2f1a6954dd2886e82f71679bc640a4a0e731b\" } cryptoScheme is used to version the encryption method and parameters used internally, like the \"salt\" value. algorithm should be \"AES256\" (the only supported algorithm). masterKeyId is the bucket's managed master encryption key ID. It is normally the identifier of the encryption key stored by the KMS server to encrypt/decrypt the individual object encryption keys for the bucket, when the user does not provide its own KMS key ID to use in the \"aws:kms\" encryption mode (and when the default bucket encryption does not contain an explicit one). It may also be the hex-encoded master encryption key itself if there is no KMS server, such as in this example (for testing purpose). Note : On AWS, the master key ID is an ARN of the form \"arn:aws:kms:region:account:key/uuid\". S3C does not follow this convention, in this example it is a \"file\" backend key (only used for testing), and key IDs look different with a real KMS server. Default Encryption Configuration With \"AES256\" Algorithm A bucket may store a default encryption configuration, set via the PutBucketEncryption API. When this happens, the bucket metadata serverSideEncryption is created or updated with the following fields: serverSideEncryption: { cryptoScheme: 1, algorithm: \"AES256\", masterKeyId: \"48c5b43c9ba57f052deac9c081b2f1a6954dd2886e82f71679bc640a4a0e731b\", mandatory: true } Here, we set the mandatory: true field to know that the configuration shall be applied by default on all new objects. The serverSideEncryption metadata attribute is created if it does not exist yet and a new master key is created via the KMS server, or if the bucket already has the serverSideEncryption attribute, it is updated by setting the mandatory: true field to enforce a default encryption on new objects, and keeping the other fields as-is, in particular the masterKeyId is not changed (we do not plan to support other encryption schemes than AES256 for now so the rest of the fields can be left alone). Default Encryption Configuration With \"aws:kms\" Algorithm In case the default encryption configuration is of type \"aws:kms\", the master key ID has been specified explicitly in the \"KMSMasterKeyId\" tag during PutBucketEncryption, and it is also stored in this section under the configuredMasterKeyId field, and will take precedence over the masterKeyId when applying the default encryption parameters to objects: serverSideEncryption: { cryptoScheme: 1, algorithm: \"aws:kms\", masterKeyId: \"48c5b43c9ba57f052deac9c081b2f1a6954dd2886e82f71679bc640a4a0e731b\", mandatory: true, configuredMasterKeyId: \"f04e1f176cfc3dd1f104921dc9e1aff82cb15c3c534ee973ff5653e32c7c439d\" } Note : this default configured master key ID to use is stored separately from the bucket managed master key ID in the serverSideEncryption section, because it can be set or removed dynamically by the bucket encryption configuration API and should not alter the existing (permanent) bucket encryption managed key that is used to encrypt objects without an explicit KMS key ID. Legacy Bucket Encryption Configuration (create_encrypted_bucket.js) For buckets that were created encrypted in previous releases using the create_encrypted_bucket.js script, the metadata format is the same as in the section Default Encryption Configuration With \"AES256\" Algorithm . This way, we can treat it in the same way, simplifying the logic and allowing to return the existing encryption configuration via GetBucketEncryption. For reference, the existing metadata for legacy encryption configuration looks like this: serverSideEncryption: { cryptoScheme: 1, algorithm: \"AES256\", masterKeyId: \"48c5b43c9ba57f052deac9c081b2f1a6954dd2886e82f71679bc640a4a0e731b\", mandatory: true } Internal Object Metadata Each encrypted object contains in its metadata a set of attributes resembling the following: \"x-amz-server-side-encryption\": \"AES256\"|\"aws:kms\", \"x-amz-server-side-encryption-aws-kms-key-id\": \"48c5b43c9ba57f052deac9c081b2f1a6954dd2886e82f71679bc640a4a0e731b\", \"x-amz-server-side-encryption-customer-algorithm\": \"\", x-amz-server-side-encryption is set to the encryption algorithm, \"AES256\", or \"aws:kms\" if there is a custom KMS key ID that should be returned when fetching object metadata (HEAD object request) x-amz-server-side-encryption-aws-kms-key-id is the identifier of the KMS master key used to encrypt/decrypt the object data encryption key, set in all encryption modes. x-amz-server-side-encryption-customer-algorithm : always set to the empty string, relates to customer-provided keys mode (SSE-C) which is not supported at the moment. Each data location object also contains the following attributes: \"location\": [ { // other location attributes: \"key\" etc. \"cryptoScheme\": 1, \"cipheredDataKey\": \"d88SLogLTw8nzRyO0Tf0qynQPLubT0N9mBDAxEZ5ww0=\" } ] cryptoScheme is used to version the encryption method and parameters used internally, like the \"salt\" value. cipheredDataKey is the object data encryption key, itself stored encrypted with the master key Bucket Policies We currently do not plan to support bucket policies related to encryption, namely, specifying conditions attached to the s3:x-amz-server-side-encryption attribute in bucket policies. The main reason to implement those would be to prevent users from using a different encryption scheme than what is set by default in the bucket encryption configuration. When a bucket has a default encryption configuration attached, it is not possible to send unencrypted objects anymore to the bucket, because: the absence of the x-amz-server-side-encryption header in a PUT object request automatically applies the default encryption to the object if present, the x-amz-server-side-encryption header must contain a valid encryption mode, there is no \"unencrypted\" mode available. For this reason, we don't support those bucket policies today.","title":"Bucket Encryption"},{"location":"design/bucket-encryption/#bucket-encryption","text":"","title":"Bucket Encryption"},{"location":"design/bucket-encryption/#old-support-for-bucket-level-encryption","text":"","title":"Old support for bucket-level encryption"},{"location":"design/bucket-encryption/#create_encrypted_bucketjs","text":"The previous way of encrypting objects in buckets is by using a script to create the bucket like: docker exec scality-s3 node bin/create_encrypted_bucket.js -h 10.10.61.61 -p 8000 -a XIJG9OTSMLDDIEWX21HT -k BTNMVTDBujqpTUEVYVYtfhxExCWy3KvDlad4hmwC -b my-encrypted-bucket The script is creating a bucket through the REST API, passing an extra proprietary header \"x-amz-scal-server-side-encryption: AES256\" to enable encryption using a newly created bucket master key stored in the KMS server, and referenced from the bucket metadata. It's also possible with the existing REST API to specify an existing master key ID to use by providing the extra \"x-amz-scal-server-side-encryption-aws-kms-key-id\" header, although the create_encrypted_bucket.js script does not provide this option. We plan to keep the compatibility with those headers for now but the compatibility will be removed eventually in favor of the bucket encryption API.","title":"create_encrypted_bucket.js"},{"location":"design/bucket-encryption/#bucket-encryption-api","text":"We now support bucket encryption using the standard S3 APIs, to enable or disable encryption on existing buckets: PutBucketEncryption GetBucketEncryption DeleteBucketEncryption","title":"Bucket Encryption API"},{"location":"design/bucket-encryption/#putbucketencryption","text":"We support the following rule to enable encryption automatically on all objects in the bucket: <ServerSideEncryptionConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"> <Rule> <ApplyServerSideEncryptionByDefault> <SSEAlgorithm>AES256</SSEAlgorithm> </ApplyServerSideEncryptionByDefault> </Rule> ... </ServerSideEncryptionConfiguration> Alternatively, we also allow specifying a master key ID to use for the bucket that will be sent when contacting the KMS via the KMSMasterKeyID tag, instead of letting S3C generate one automatically: <ServerSideEncryptionConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"> <Rule> <ApplyServerSideEncryptionByDefault> <SSEAlgorithm>aws:kms</SSEAlgorithm> <KMSMasterKeyID>abcdef</KMSMasterKeyID> </ApplyServerSideEncryptionByDefault> </Rule> ... </ServerSideEncryptionConfiguration> Note : only one <Rule> entry is allowed in the server-side encryption configuration. Note : the <BucketKeyEnabled> rule option is not supported.","title":"PutBucketEncryption"},{"location":"design/bucket-encryption/#implementation-of-putbucketencryption-regarding-bucket-metadata","text":"If the serverSideEncryption does not exist, we create it and fill with the relevant attributes (see Internal Bucket Metadata ) If the serverSideEncryption exists, we set its mandatory attribute to true , and possibly set the algorithm to the new configured algorithm, and if it is aws:kms , the KMSMasterKeyId also gets stored in the configuredMasterKeyId attribute. We do not change the other attributes, notably the masterKeyId is not replaced by a new one.","title":"Implementation of PutBucketEncryption Regarding Bucket Metadata"},{"location":"design/bucket-encryption/#getbucketencryption","text":"Returns the bucket encryption configuration, as specified by a previous PutBucketEncryption. Note : due to the bucket metadata format preservation, it should also be returning an existing bucket-level encryption configuration for buckets created with the legacy x-amz-scal-server-side-encryption header, that did not get an explicit configuration applied via PutBucketEncryption.","title":"GetBucketEncryption"},{"location":"design/bucket-encryption/#implementation-of-getbucketencryption-regarding-bucket-metadata","text":"See Internal Bucket Metadata The implementation reads the fields stored in the serverSideEncryption bucket metadata attribute, and returns the formatted XML response filled with the needed information. If there is no serverSideEncryption attribute, or if the serverSideEncryption attribute does not contain the mandatory: true field, return ServerSideEncryptionConfigurationNotFoundError .","title":"Implementation of GetBucketEncryption Regarding Bucket Metadata"},{"location":"design/bucket-encryption/#deletebucketencryption","text":"Deletes the existing bucket encryption configuration, removing any default bucket encryption policy for the bucket hence storing new objects unencrypted unless the requests provide the header x-amz-server-side-encryption . Internally, some of the bucket encryption metadata will be kept but the mandatory flag is removed (see Internal Metadata section).","title":"DeleteBucketEncryption"},{"location":"design/bucket-encryption/#implementation-of-deletebucketencryption-regarding-bucket-metadata","text":"See Internal Bucket Metadata When the bucket encryption configuration is deleted, we don't entirely delete the serverSideEncryption section, because we want to keep the existing parameters, notably the masterKeyId . Instead, we drop the mandatory field and configuredMasterKeyId (if set), and keep the other fields. Deleting a bucket encryption configuration that does not exist (i.e. no serverSideEncryption section or its mandatory field is unset) returns a silent success to the client.","title":"Implementation of DeleteBucketEncryption Regarding Bucket Metadata"},{"location":"design/bucket-encryption/#application-logic-for-object-encryption","text":"The following steps determine when and how to apply encryption to a particular object PUT request: Does the request contain the x-amz-server-side-encryption header? yes : encrypt object with the algorithm value of x-amz-server-side-encryption and master key ID x-amz-server-side-encryption-aws-kms-key-id if it is set and the algorithm value is aws:kms , or the bucket managed key under serverSideEncryption.masterKeyId otherwise no : continue Does the bucket have a default encryption configuration? (i.e the bucket metadata field serverSideEncryption.mandatory is true ) yes : encrypt object with the algorithm value of serverSideEncryption.algorithm and either the serverSideEncryption.configuredMasterKeyId if it is set, or the master key ID serverSideEncryption.masterKeyId otherwise no : do not encrypt the object The following steps give the procedure when an object is to be encrypted: if the algorithm value is AES256 : create server-side bucket encryption parameters if they do not exist yet in the serverSideEncryption bucket metadata field: set the algorithm attribute to AES256 and the masterKeyId to a new master key ID for that bucket, as well as cryptoScheme with value 1 . Do not set the mandatory field, as this configuration shall not apply automatically to new objects. encrypt the object with a new data key, itself encrypted with the bucket master key store the encrypted data key and master key ID (the same as the bucket master key ID) in the object metadata if the algorithm value is aws:kms and a master key ID is provided: encrypt the object with a new data key, itself encrypted with the user-specified master key store the encrypted data key and user-specified master key ID in the object metadata. Do not create or alter the bucket server-side encryption parameters. if the algorithm value is aws:kms but no master key ID is provided, follow the AES256 logic above.","title":"Application Logic for Object Encryption"},{"location":"design/bucket-encryption/#internal-metadata","text":"The internal metadata format should not change with the introduction of encryption API support.","title":"Internal Metadata"},{"location":"design/bucket-encryption/#internal-bucket-metadata","text":"The internal bucket metadata related to encryption is stored under the serverSideEncryption attribute. It may be of three slightly different formats, described below. Note : The serverSideEncryption field used to be created automatically when the x-amz-scal-server-side-encryption was provided at bucket creation time. It is now created dynamically, either when a default bucket encryption configuration is applied to the bucket or when the first object requesting encryption (through the x-amz-server-side-encryption header) is stored in the bucket.","title":"Internal Bucket Metadata"},{"location":"design/bucket-encryption/#managed-kms-master-key-without-a-default-configuration","text":"When a bucket has a managed encryption key associated to it, but no default encryption applied to new objects automatically, its metadata serverSideEncryption looks like this: serverSideEncryption: { cryptoScheme: 1, algorithm: \"AES256\", masterKeyId: \"48c5b43c9ba57f052deac9c081b2f1a6954dd2886e82f71679bc640a4a0e731b\" } cryptoScheme is used to version the encryption method and parameters used internally, like the \"salt\" value. algorithm should be \"AES256\" (the only supported algorithm). masterKeyId is the bucket's managed master encryption key ID. It is normally the identifier of the encryption key stored by the KMS server to encrypt/decrypt the individual object encryption keys for the bucket, when the user does not provide its own KMS key ID to use in the \"aws:kms\" encryption mode (and when the default bucket encryption does not contain an explicit one). It may also be the hex-encoded master encryption key itself if there is no KMS server, such as in this example (for testing purpose). Note : On AWS, the master key ID is an ARN of the form \"arn:aws:kms:region:account:key/uuid\". S3C does not follow this convention, in this example it is a \"file\" backend key (only used for testing), and key IDs look different with a real KMS server.","title":"Managed KMS Master Key Without a Default Configuration"},{"location":"design/bucket-encryption/#default-encryption-configuration-with-aes256-algorithm","text":"A bucket may store a default encryption configuration, set via the PutBucketEncryption API. When this happens, the bucket metadata serverSideEncryption is created or updated with the following fields: serverSideEncryption: { cryptoScheme: 1, algorithm: \"AES256\", masterKeyId: \"48c5b43c9ba57f052deac9c081b2f1a6954dd2886e82f71679bc640a4a0e731b\", mandatory: true } Here, we set the mandatory: true field to know that the configuration shall be applied by default on all new objects. The serverSideEncryption metadata attribute is created if it does not exist yet and a new master key is created via the KMS server, or if the bucket already has the serverSideEncryption attribute, it is updated by setting the mandatory: true field to enforce a default encryption on new objects, and keeping the other fields as-is, in particular the masterKeyId is not changed (we do not plan to support other encryption schemes than AES256 for now so the rest of the fields can be left alone).","title":"Default Encryption Configuration With \"AES256\" Algorithm"},{"location":"design/bucket-encryption/#default-encryption-configuration-with-awskms-algorithm","text":"In case the default encryption configuration is of type \"aws:kms\", the master key ID has been specified explicitly in the \"KMSMasterKeyId\" tag during PutBucketEncryption, and it is also stored in this section under the configuredMasterKeyId field, and will take precedence over the masterKeyId when applying the default encryption parameters to objects: serverSideEncryption: { cryptoScheme: 1, algorithm: \"aws:kms\", masterKeyId: \"48c5b43c9ba57f052deac9c081b2f1a6954dd2886e82f71679bc640a4a0e731b\", mandatory: true, configuredMasterKeyId: \"f04e1f176cfc3dd1f104921dc9e1aff82cb15c3c534ee973ff5653e32c7c439d\" } Note : this default configured master key ID to use is stored separately from the bucket managed master key ID in the serverSideEncryption section, because it can be set or removed dynamically by the bucket encryption configuration API and should not alter the existing (permanent) bucket encryption managed key that is used to encrypt objects without an explicit KMS key ID.","title":"Default Encryption Configuration With \"aws:kms\" Algorithm"},{"location":"design/bucket-encryption/#legacy-bucket-encryption-configuration-create_encrypted_bucketjs","text":"For buckets that were created encrypted in previous releases using the create_encrypted_bucket.js script, the metadata format is the same as in the section Default Encryption Configuration With \"AES256\" Algorithm . This way, we can treat it in the same way, simplifying the logic and allowing to return the existing encryption configuration via GetBucketEncryption. For reference, the existing metadata for legacy encryption configuration looks like this: serverSideEncryption: { cryptoScheme: 1, algorithm: \"AES256\", masterKeyId: \"48c5b43c9ba57f052deac9c081b2f1a6954dd2886e82f71679bc640a4a0e731b\", mandatory: true }","title":"Legacy Bucket Encryption Configuration (create_encrypted_bucket.js)"},{"location":"design/bucket-encryption/#internal-object-metadata","text":"Each encrypted object contains in its metadata a set of attributes resembling the following: \"x-amz-server-side-encryption\": \"AES256\"|\"aws:kms\", \"x-amz-server-side-encryption-aws-kms-key-id\": \"48c5b43c9ba57f052deac9c081b2f1a6954dd2886e82f71679bc640a4a0e731b\", \"x-amz-server-side-encryption-customer-algorithm\": \"\", x-amz-server-side-encryption is set to the encryption algorithm, \"AES256\", or \"aws:kms\" if there is a custom KMS key ID that should be returned when fetching object metadata (HEAD object request) x-amz-server-side-encryption-aws-kms-key-id is the identifier of the KMS master key used to encrypt/decrypt the object data encryption key, set in all encryption modes. x-amz-server-side-encryption-customer-algorithm : always set to the empty string, relates to customer-provided keys mode (SSE-C) which is not supported at the moment. Each data location object also contains the following attributes: \"location\": [ { // other location attributes: \"key\" etc. \"cryptoScheme\": 1, \"cipheredDataKey\": \"d88SLogLTw8nzRyO0Tf0qynQPLubT0N9mBDAxEZ5ww0=\" } ] cryptoScheme is used to version the encryption method and parameters used internally, like the \"salt\" value. cipheredDataKey is the object data encryption key, itself stored encrypted with the master key","title":"Internal Object Metadata"},{"location":"design/bucket-encryption/#bucket-policies","text":"We currently do not plan to support bucket policies related to encryption, namely, specifying conditions attached to the s3:x-amz-server-side-encryption attribute in bucket policies. The main reason to implement those would be to prevent users from using a different encryption scheme than what is set by default in the bucket encryption configuration. When a bucket has a default encryption configuration attached, it is not possible to send unencrypted objects anymore to the bucket, because: the absence of the x-amz-server-side-encryption header in a PUT object request automatically applies the default encryption to the object if present, the x-amz-server-side-encryption header must contain a valid encryption mode, there is no \"unencrypted\" mode available. For this reason, we don't support those bucket policies today.","title":"Bucket Policies"},{"location":"design/bucket-notification/","text":"Bucket Notification The bucket notification feature enables an external application to receive notifications when certain events happen in a bucket. The API part of the feature will be built in line with the AWS specification while the notification delivery system will be specific to our solution, and will permit the delivery through a variety of mediums (e.g. RabbitMQ, Kafka, etc). We plan to leverage the Backbeat component to perform the collection of events from the S3C Raft oplog (and later on from MongoDB oplog), the queuing and the delivery of events. The feature has to be implemented mainly for S3C and eventually be implemented for Zenko and Blobserver. Requirements Use-cases Description API The PutBucketNotificationConfiguration API will store the bucket configuration in a post-processed way (XML converted to JSON) into the metastore special database. The structure of a BucketNotificationConfiguration payload looks like the one defined in the AWS spec but is slightly different: In the AWS spec it is possible to specify: TopicConfiguration: for notifying an event (SNS) QueueConfiguration: for queueing an event (SQS) CloudFunctionConfiguration: for executing a cloud function (Lambda) We plan to only support QueueConfiguration for now. For each QueueConfiguration there are 4 types of items: Id : a unique user defined ID or automatically generated. Event : the type of event. Optional Filter : define specific sub-filter rules for objects such as prefix and suffix . If not defined it will generate an event for all objects. QueueArn : the ARN of the target (see thereafter). Example of a notification configuration (for a given bucket): <NotificationConfiguration> <QueueConfiguration> <Id>MyFilterSet1</Id> <Event>s3:ObjectCreated:*</Event> <QueueArn>arn:scality:bucketnotif:::target1</QueueArn> </QueueConfiguration> <QueueConfiguration> <Id>MyFilterSet2</Id> <Event>s3:ObjectCreated:*</Event> <Filter> <S3Key> <FilterRule> <Name>prefix</Name> <Value>images</Value> </FilterRule> <FilterRule> <Name>suffix</Name> <Value>.jpg</Value> </FilterRule> </S3Key> </Filter> <QueueArn>arn:scality:bucketnotif:::target2</QueueArn> </QueueConfiguration> </NotificationConfiguration> Event types that will need to support: Event Description s3:ObjectCreated:* , s3:ObjectCreated:Put , s3:ObjectCreated:Copy , s3:ObjectCreated:CompleteMultipartUpload Will generate an event respectively every time an object is created (whatever the operation type), written, copied, MPU constituted. s3:ObjectRemoved:* , s3:ObjectRemoved:Delete , s3:ObjectRemoved:DeleteMarkerCreated Will generate an event respectively every time an object is deleted (whatever the operation type), deleted, or a delete marker is created. Event types that will need to support eventually (in future releases): Event Description s3:ObjectCreated:Post Will generate an event every time an object is created. s3:ObjectRestore:Post , s3:ObjectRestore:Completed Will generate an event when an object will be restored from an archival storage (e.g. tape, or Glacier) s3:Replication:OperationFailedReplication , s3:Replication:OperationMissedThreshold , s3:Replication:OperationReplicatedAfterThreshold , s3:Replication:OperationNotTracked Specific replication related events. Since it will be exhausting to redefine the entire SQS API, we define our own ARN format for the targets ( QueueArn ) that will map directly to static config: arn:partition:service:region:account-id:resource With the following ARN mapping: partition being hardcoded scality . service being harcoded bucketnotif . region being ignored. account-id being ignored. resource being the name of the static target name (see below). In S3C the targets will be specified directly in the group_vars/all as YAML definitions, e.g.: env_bucket_notifications: - resource: target1 type: rabbitmq host: <somehost> port: <someport> auth: - user: <user> password: <password> - resource: target2 type: kafka host: <somehost> port: auth: - cert: certificate path For the first version of the service we store the secrets in the configuration. In Zenko the targets will be defined in ConfigMaps and Secrets. Both Cloudserver and Backbeat need to be aware of the targets configurations. Technical Details Cloudserver Changes In the PutBucketNotificationConfiguration API, Cloudserver must check that the ARN points to a valid target. Arsenal needs to be modified to store the operation that generated the event in the ObjectMD structure. For this a new field originOp will be added in ObjectMD. E.g. it may be structured like this: originOp: copy , originOp: delete . We so far stick to the AWS event name specification as it is broad enough (we may later extend it for more granularity). Backbeat Changes A specific database oplog consumer (here Metadata oplog, but later MongoDB oplog) will filter out the metadata operations according to the filters defined in the bucket configuration, interpret the originOp field and queue the operations in a specific Backbeat topic bucket-notification . The bucket notification queue populator will live in its own pod. We plan to use the LogReader class in Backbeat to build the bucket notification S3C oplog reader. We plan to leverage this class since it already abstracts the different oplogs mechanisms in a unified interface. There could be slight changes into the LogReader class as it used to be meant for the QueuePopulationExtension mechanism that we are going to deprecate. +---------+ |Metadata/| |MongoDB | +-----------+ +------+--+ |Replication| | oplog |Processor | | +----+------+ | +---+pod+-------+ | +-+pod+------+ +-----------------+ +------>+Bucket notif | +--v---+ |Bucket notif| |external endpoint| |queue populator+-->+Topic +<---+Processor +--->+Kafka, RabbitMQ | +------+--------+ +------+ +----+-------+ +-----------------+ | filter | refilter | | | +---------- | +---------->Zookeeper+<-------+ target +---------+ read config config Note that in order to support s3:replication related events we will need to have the replication processor directly writing events in the Bucket Notification Topic. Having each service (Replication, Ingestion, Bucket notification, Workflow engine, etc) in its own pod allows better separation of concerns in term of: configuring, scaling, restarting the services. Dynamic Filters Configuration The Bucket notification queue populator will get the current FilterRules directly from the metastore related oplog events, indiscriminately for the initial discovery and for the dynamic changes (FilterRules added or removed). Note: For S3C listening to all the metastore oplog events is scalable because it has its own raft session and we don't expect to have too many entries. In the case of Zenko, we may have to do an initial discovery directly into the metastore bucket, and then the oplog is rotated so we expect it to be relativelty fast to process. For each NotificationConfiguration detected we will update a specific Zookeeper entry in the path /com/scality/backbeat/bucket-notification . E.g. /com/scality/backbeat/bucket-notification/<BucketName>/MyFilterSet1 where BucketName is the name of the bucket, MyFilterSet is the unique id defined in the QueueConfiguration . The file will contain all the filter rules and the target in JSON format. Bucket Notification Queue Populator: Event Processing Each time the Bucket notification queue populator will match an event in the oplog according to the active set of filters, it will create an entry in the Bucket Notification topic. For sake of disk space we only have one global topic for the service, will only record the bucket name, object name and type of event (and strip out the metadata of the object). The bucket notification processor will have to do a refilter (see next section) to avoid tagging the entries. This also have the advantage of having the processor reflecting immediately the dynamic configuration changes and not having a \"lag\". Notes for buckets formats and versioning: For versioned buckets we need to ignore the master since there are always 2 updates in a batch. We also need to properly manage buckets where versioned has been disabled. We also need to manage the new bucket format, by potentially skipping the prefix. Bucket Notification Processor: Actions The bucket notification processor will adapt dynamically to bucket notification configuration changes (added / deleted filter rules) by setting a watcher on the bucket notification Zookeeper path. Each target will have its own processor on the topic, consume at its own pace, skip entries which do not match and produce in its external endpoint. Parallelism will be achieved by having consumer groups. Note: When delivering events to the external Kafka for instance, we need to be aware that the acknowledgment arrives in the delivery report event and not in the callback of the message. To manage this problem we can leverage the OffsetLedger class as it has been used in the replication mechanism. Retries can be done in memory and persistence is done when acknowledging offsets in the commit. Note that a message can be delivered more than once but this is not a problem since the goal is to deliver the message at least once. Pause & Resume TBD Monitoring & Metrics TBD UI The S3 Browser UI needs to be adapted to this new feature, and should target the following user story: As a Data Consumer, I want to configure a set of rules to publish notification messages in my queuing systems when activity occurs on a bucket so that I can monitor data-changes within that particular buckets. The main change will occurs in the View Bucket Info Modal. Three tabs will be added: Overview will provide a summary of the bucket configuration. Object Lock will provide the ability to configure Object Lock feature. Notification will provide the ability to configure Bucket Notification feature. Overview Tab Acceptance Criteria: * The overview tab shall contain the old Overview section, and the Permissions section. * The old Overview section name shall be replaced by Properties . Object Lock Tab Acceptance Criteria: * The Notification tab shall contain the old Object Lock Default Settings section. * The old Object Lock Default Settings section name shall be replaced by Default Settings . Notification Tab Acceptance criteria: * The bucket notification configuration must be retrieved and displayed according to the GetBucketNotificationConfiguration action. * Clicking ADD NOTIFICATION button opens add notification modal . * Clicking Edit button opens edit notification modal . * Clicking DELETE button shall delete all selected rules from the bucket notification using PutBucketNotificationConfiguration action. Add and Edit Notification Modal Acceptance criteria: * Pressing the tab key must switch the focus between fields. * If s3:ObjectCreated:* event is selected, then s3:ObjectCreated:Put , s3:ObjectCreated:Post , s3:ObjectCreated:Copy , and s3:ObjectCreated:CompleteMultipartUpload events can't be checked. * If s3:ObjectRemoved:* event is selected, then s3:ObjectRemoved:Delete , and s3:ObjectRemoved:DeleteMarkerCreated events can't be checked. * Clicking ADD NOTIFICATION RULE and EDIT NOTIFICATION RULE shall update the bucket notification configuration according to inputs using PutBucketNotificationConfiguration action. Alternatives We could have leveraged tabs on the main pages to avoid modal, and provide a better UX. But, the development cost of that change is not compatible with our deadlines, and the S3 Browser will be replaced by XDM UI offline in 2021. List of Improvements Suggest the ARNs during the bucket notification configuration. Provide the ability to create the destination queues. Alternatives We could have leveraged the QueuePopulatorExtension mechanism but we decided not to. This mechanism relies on the idea that having multiple extensions querying the same oplog will overload the source metadata engine (as this kind of problems appeared especially on S3C repds). +---------+ +---pod--------+ |Metadata/|oplog|QueuePopulator| |MongoDB +---->+ | +---------+ +--------------+ +------------+ +---pod------+ |QueuePopulator| |Replication | |Replication | replication |Extension 1 +---->Topic <-------+Processor +-------------> |Replication | |(w/ attr) | |(Consumer) | +--------------+ +------------+ +------------+ |Workflow | |Engine Queue | +------------+ +---pod----+ |Populator +--->+Workflow | |Workflow | actions +----------+---+ |(uuid) <-------+engine +------------> ^ |(w/ attr or | |processor | | |w/o attr) | +-+--------+ | +------------+ | | | | set filter descriptors (ZK) | +-------------------------------+ But those load issues have been fixed on S3C so far. The oplog management have been correctly abstracted in a class in Backbeat so we think it is better to benefit from a clean separation of concerns. We think that the concept of QueuePopulatorExtension leads to more complex designs (for the workflow engine we had to invent a complex way to reconfigure the filters without restarting the replication queue populator). We also noticed that the ingestion mechanism lives in its own pod but still uses the queuePopulatorExtension mechanism by itself, leading to a code which is unnecessarily complex. So later on we plan to simplify this code. Same for the Workflow engine queue populator that could also live in its pod in using the LogReader classes directly. We also plan to remove the QueuePopulatorExtension from the code.","title":"Bucket Notifications"},{"location":"design/bucket-notification/#bucket-notification","text":"The bucket notification feature enables an external application to receive notifications when certain events happen in a bucket. The API part of the feature will be built in line with the AWS specification while the notification delivery system will be specific to our solution, and will permit the delivery through a variety of mediums (e.g. RabbitMQ, Kafka, etc). We plan to leverage the Backbeat component to perform the collection of events from the S3C Raft oplog (and later on from MongoDB oplog), the queuing and the delivery of events. The feature has to be implemented mainly for S3C and eventually be implemented for Zenko and Blobserver.","title":"Bucket Notification"},{"location":"design/bucket-notification/#requirements","text":"","title":"Requirements"},{"location":"design/bucket-notification/#use-cases-description","text":"","title":"Use-cases Description"},{"location":"design/bucket-notification/#api","text":"The PutBucketNotificationConfiguration API will store the bucket configuration in a post-processed way (XML converted to JSON) into the metastore special database. The structure of a BucketNotificationConfiguration payload looks like the one defined in the AWS spec but is slightly different: In the AWS spec it is possible to specify: TopicConfiguration: for notifying an event (SNS) QueueConfiguration: for queueing an event (SQS) CloudFunctionConfiguration: for executing a cloud function (Lambda) We plan to only support QueueConfiguration for now. For each QueueConfiguration there are 4 types of items: Id : a unique user defined ID or automatically generated. Event : the type of event. Optional Filter : define specific sub-filter rules for objects such as prefix and suffix . If not defined it will generate an event for all objects. QueueArn : the ARN of the target (see thereafter). Example of a notification configuration (for a given bucket): <NotificationConfiguration> <QueueConfiguration> <Id>MyFilterSet1</Id> <Event>s3:ObjectCreated:*</Event> <QueueArn>arn:scality:bucketnotif:::target1</QueueArn> </QueueConfiguration> <QueueConfiguration> <Id>MyFilterSet2</Id> <Event>s3:ObjectCreated:*</Event> <Filter> <S3Key> <FilterRule> <Name>prefix</Name> <Value>images</Value> </FilterRule> <FilterRule> <Name>suffix</Name> <Value>.jpg</Value> </FilterRule> </S3Key> </Filter> <QueueArn>arn:scality:bucketnotif:::target2</QueueArn> </QueueConfiguration> </NotificationConfiguration> Event types that will need to support: Event Description s3:ObjectCreated:* , s3:ObjectCreated:Put , s3:ObjectCreated:Copy , s3:ObjectCreated:CompleteMultipartUpload Will generate an event respectively every time an object is created (whatever the operation type), written, copied, MPU constituted. s3:ObjectRemoved:* , s3:ObjectRemoved:Delete , s3:ObjectRemoved:DeleteMarkerCreated Will generate an event respectively every time an object is deleted (whatever the operation type), deleted, or a delete marker is created. Event types that will need to support eventually (in future releases): Event Description s3:ObjectCreated:Post Will generate an event every time an object is created. s3:ObjectRestore:Post , s3:ObjectRestore:Completed Will generate an event when an object will be restored from an archival storage (e.g. tape, or Glacier) s3:Replication:OperationFailedReplication , s3:Replication:OperationMissedThreshold , s3:Replication:OperationReplicatedAfterThreshold , s3:Replication:OperationNotTracked Specific replication related events. Since it will be exhausting to redefine the entire SQS API, we define our own ARN format for the targets ( QueueArn ) that will map directly to static config: arn:partition:service:region:account-id:resource With the following ARN mapping: partition being hardcoded scality . service being harcoded bucketnotif . region being ignored. account-id being ignored. resource being the name of the static target name (see below). In S3C the targets will be specified directly in the group_vars/all as YAML definitions, e.g.: env_bucket_notifications: - resource: target1 type: rabbitmq host: <somehost> port: <someport> auth: - user: <user> password: <password> - resource: target2 type: kafka host: <somehost> port: auth: - cert: certificate path For the first version of the service we store the secrets in the configuration. In Zenko the targets will be defined in ConfigMaps and Secrets. Both Cloudserver and Backbeat need to be aware of the targets configurations.","title":"API"},{"location":"design/bucket-notification/#technical-details","text":"","title":"Technical Details"},{"location":"design/bucket-notification/#cloudserver-changes","text":"In the PutBucketNotificationConfiguration API, Cloudserver must check that the ARN points to a valid target. Arsenal needs to be modified to store the operation that generated the event in the ObjectMD structure. For this a new field originOp will be added in ObjectMD. E.g. it may be structured like this: originOp: copy , originOp: delete . We so far stick to the AWS event name specification as it is broad enough (we may later extend it for more granularity).","title":"Cloudserver Changes"},{"location":"design/bucket-notification/#backbeat-changes","text":"A specific database oplog consumer (here Metadata oplog, but later MongoDB oplog) will filter out the metadata operations according to the filters defined in the bucket configuration, interpret the originOp field and queue the operations in a specific Backbeat topic bucket-notification . The bucket notification queue populator will live in its own pod. We plan to use the LogReader class in Backbeat to build the bucket notification S3C oplog reader. We plan to leverage this class since it already abstracts the different oplogs mechanisms in a unified interface. There could be slight changes into the LogReader class as it used to be meant for the QueuePopulationExtension mechanism that we are going to deprecate. +---------+ |Metadata/| |MongoDB | +-----------+ +------+--+ |Replication| | oplog |Processor | | +----+------+ | +---+pod+-------+ | +-+pod+------+ +-----------------+ +------>+Bucket notif | +--v---+ |Bucket notif| |external endpoint| |queue populator+-->+Topic +<---+Processor +--->+Kafka, RabbitMQ | +------+--------+ +------+ +----+-------+ +-----------------+ | filter | refilter | | | +---------- | +---------->Zookeeper+<-------+ target +---------+ read config config Note that in order to support s3:replication related events we will need to have the replication processor directly writing events in the Bucket Notification Topic. Having each service (Replication, Ingestion, Bucket notification, Workflow engine, etc) in its own pod allows better separation of concerns in term of: configuring, scaling, restarting the services.","title":"Backbeat Changes"},{"location":"design/bucket-notification/#dynamic-filters-configuration","text":"The Bucket notification queue populator will get the current FilterRules directly from the metastore related oplog events, indiscriminately for the initial discovery and for the dynamic changes (FilterRules added or removed). Note: For S3C listening to all the metastore oplog events is scalable because it has its own raft session and we don't expect to have too many entries. In the case of Zenko, we may have to do an initial discovery directly into the metastore bucket, and then the oplog is rotated so we expect it to be relativelty fast to process. For each NotificationConfiguration detected we will update a specific Zookeeper entry in the path /com/scality/backbeat/bucket-notification . E.g. /com/scality/backbeat/bucket-notification/<BucketName>/MyFilterSet1 where BucketName is the name of the bucket, MyFilterSet is the unique id defined in the QueueConfiguration . The file will contain all the filter rules and the target in JSON format.","title":"Dynamic Filters Configuration"},{"location":"design/bucket-notification/#bucket-notification-queue-populator-event-processing","text":"Each time the Bucket notification queue populator will match an event in the oplog according to the active set of filters, it will create an entry in the Bucket Notification topic. For sake of disk space we only have one global topic for the service, will only record the bucket name, object name and type of event (and strip out the metadata of the object). The bucket notification processor will have to do a refilter (see next section) to avoid tagging the entries. This also have the advantage of having the processor reflecting immediately the dynamic configuration changes and not having a \"lag\". Notes for buckets formats and versioning: For versioned buckets we need to ignore the master since there are always 2 updates in a batch. We also need to properly manage buckets where versioned has been disabled. We also need to manage the new bucket format, by potentially skipping the prefix.","title":"Bucket Notification Queue Populator: Event Processing"},{"location":"design/bucket-notification/#bucket-notification-processor-actions","text":"The bucket notification processor will adapt dynamically to bucket notification configuration changes (added / deleted filter rules) by setting a watcher on the bucket notification Zookeeper path. Each target will have its own processor on the topic, consume at its own pace, skip entries which do not match and produce in its external endpoint. Parallelism will be achieved by having consumer groups. Note: When delivering events to the external Kafka for instance, we need to be aware that the acknowledgment arrives in the delivery report event and not in the callback of the message. To manage this problem we can leverage the OffsetLedger class as it has been used in the replication mechanism. Retries can be done in memory and persistence is done when acknowledging offsets in the commit. Note that a message can be delivered more than once but this is not a problem since the goal is to deliver the message at least once.","title":"Bucket Notification Processor: Actions"},{"location":"design/bucket-notification/#pause-resume","text":"TBD","title":"Pause &amp; Resume"},{"location":"design/bucket-notification/#monitoring-metrics","text":"TBD","title":"Monitoring &amp; Metrics"},{"location":"design/bucket-notification/#ui","text":"The S3 Browser UI needs to be adapted to this new feature, and should target the following user story: As a Data Consumer, I want to configure a set of rules to publish notification messages in my queuing systems when activity occurs on a bucket so that I can monitor data-changes within that particular buckets. The main change will occurs in the View Bucket Info Modal. Three tabs will be added: Overview will provide a summary of the bucket configuration. Object Lock will provide the ability to configure Object Lock feature. Notification will provide the ability to configure Bucket Notification feature.","title":"UI"},{"location":"design/bucket-notification/#overview-tab","text":"Acceptance Criteria: * The overview tab shall contain the old Overview section, and the Permissions section. * The old Overview section name shall be replaced by Properties .","title":"Overview Tab"},{"location":"design/bucket-notification/#object-lock-tab","text":"Acceptance Criteria: * The Notification tab shall contain the old Object Lock Default Settings section. * The old Object Lock Default Settings section name shall be replaced by Default Settings .","title":"Object Lock Tab"},{"location":"design/bucket-notification/#notification-tab","text":"Acceptance criteria: * The bucket notification configuration must be retrieved and displayed according to the GetBucketNotificationConfiguration action. * Clicking ADD NOTIFICATION button opens add notification modal . * Clicking Edit button opens edit notification modal . * Clicking DELETE button shall delete all selected rules from the bucket notification using PutBucketNotificationConfiguration action.","title":"Notification Tab"},{"location":"design/bucket-notification/#add-and-edit-notification-modal","text":"Acceptance criteria: * Pressing the tab key must switch the focus between fields. * If s3:ObjectCreated:* event is selected, then s3:ObjectCreated:Put , s3:ObjectCreated:Post , s3:ObjectCreated:Copy , and s3:ObjectCreated:CompleteMultipartUpload events can't be checked. * If s3:ObjectRemoved:* event is selected, then s3:ObjectRemoved:Delete , and s3:ObjectRemoved:DeleteMarkerCreated events can't be checked. * Clicking ADD NOTIFICATION RULE and EDIT NOTIFICATION RULE shall update the bucket notification configuration according to inputs using PutBucketNotificationConfiguration action.","title":"Add and Edit Notification Modal"},{"location":"design/bucket-notification/#alternatives","text":"We could have leveraged tabs on the main pages to avoid modal, and provide a better UX. But, the development cost of that change is not compatible with our deadlines, and the S3 Browser will be replaced by XDM UI offline in 2021.","title":"Alternatives"},{"location":"design/bucket-notification/#list-of-improvements","text":"Suggest the ARNs during the bucket notification configuration. Provide the ability to create the destination queues.","title":"List of Improvements"},{"location":"design/bucket-notification/#alternatives_1","text":"We could have leveraged the QueuePopulatorExtension mechanism but we decided not to. This mechanism relies on the idea that having multiple extensions querying the same oplog will overload the source metadata engine (as this kind of problems appeared especially on S3C repds). +---------+ +---pod--------+ |Metadata/|oplog|QueuePopulator| |MongoDB +---->+ | +---------+ +--------------+ +------------+ +---pod------+ |QueuePopulator| |Replication | |Replication | replication |Extension 1 +---->Topic <-------+Processor +-------------> |Replication | |(w/ attr) | |(Consumer) | +--------------+ +------------+ +------------+ |Workflow | |Engine Queue | +------------+ +---pod----+ |Populator +--->+Workflow | |Workflow | actions +----------+---+ |(uuid) <-------+engine +------------> ^ |(w/ attr or | |processor | | |w/o attr) | +-+--------+ | +------------+ | | | | set filter descriptors (ZK) | +-------------------------------+ But those load issues have been fixed on S3C so far. The oplog management have been correctly abstracted in a class in Backbeat so we think it is better to benefit from a clean separation of concerns. We think that the concept of QueuePopulatorExtension leads to more complex designs (for the workflow engine we had to invent a complex way to reconfigure the filters without restarting the replication queue populator). We also noticed that the ingestion mechanism lives in its own pod but still uses the queuePopulatorExtension mechanism by itself, leading to a code which is unnecessarily complex. So later on we plan to simplify this code. Same for the Workflow engine queue populator that could also live in its pod in using the LogReader classes directly. We also plan to remove the QueuePopulatorExtension from the code.","title":"Alternatives"},{"location":"design/object-lock/","text":"Object Locking using S3 Object Lock Object locking is a feature built for addressing use cases in which the Write Once Read Many (WORM) model is required. The feature will be built in line with the AWS specification. Any extensions to the specification will be explicitly documented. The feature implementation has the end goal of meeting SEC 17a-4 compliance. Requirements Object lock flag must be set during bucket creation Versioning has to be enabled on the bucket (it is enabled automatically at creation time when object lock flag is set) Object lock must be enabled on a bucket in order to write a lock configuration using the PUT Object Lock Configuration api that has object lock flag set PS: AWS S3 specification does not have a way of setting object lock on existing buckets. To enable object lock flag on an existing bucket, a tool provided by Scality can be used see migration tool section . Objects that were created before the lock is set are not protected by Object Locking. What happens when an object is locked When set on an object version, a lock prevents deletes and overwrites on that version until the lock expires. However, the lock doesn't prevent creation of delete markers or new versions on top of the locked version. Object locking also protects objects from Lifecycle actions, i.e a Lifecycle expiry rule cannot delete an object version until the lock on the object expires. Controlling locking of an object S3 provides a few ways through which the lock configuration of an object can be set Retention Modes Both Governance and Compliance modes retain the lock on an object until the set retention period expires. Governance mode In addition to preventing deletions on an object version by users, GOVERNANCE mode allows delegating permission to certain users to override the lock settings, for example by changing the retention mode, period or deleting the object altogether. Either the root (account) or a user with s3:BypassGovernanceRetention permission can send a delete request with x-amz-bypass-governance-retention:true header to override and delete the object. Compliance mode When a lock is placed on an object using COMPLIANCE mode, the object version cannot be deleted by any user until the retention period expires. This includes root user (account credentials) in the account and no other user can be given permission either to override the settings or delete the version. Retention Period Retention period defines the term during which the object is protected from deletes. Retention period can be set on the bucket level, which acts as a default for all objects put in the bucket after the setting is applied. The default retention period set on a bucket can be overridden on the object level by setting the date and time using the header x-amz-object-lock-retain-until-date . Buckets - retention period can be set in either days or years , but not both at the same time. Objects - retention period can be set as date and time when the object is expected to expire. Legal Hold Legal hold can be enabled on an object version. Once a legal hold is enabled, regardless of the object's retention date or retention mode, the object version cannot be deleted until the legal hold is removed. Legal hold can be set on an object version during PUT Object request by setting x-amz-object-lock-legal-hold header or using PUT Object Legal Hold API request. Root users with account credentials or IAM users who are given the permission s3:PutObjectLegalHold are allowed to set Legal hold on an object version. Implementation Storing Object lock configuration Object lock can be enabled on a bucket during bucket creation using x-amz-bucket-object-lock-enabled header. This is stored along with the bucket's metadata defined in the BucketInfo model. Any default lock settings (governance/compliance mode, days/years for retention period) defined using PutObjectLockConfiguration request are stored as part of the bucket metadata. When a PUT Object request is received, the lock configuration is evaluated and the lock expiration date and time is calculated and set as retain-until-date property on the object's metadata. Lock configuration on a PUT object request is evaluated in the following order: Object's configuration in the PUT Object for retention mode/period Bucket's configuration for retention mode/period The object's settings override the bucket's settings when calculating the retain-until-date date and time to be stored on the object's metadata. The lock configuration on an object version can also be changed using PutObjectRetention api request if the user has appropriate permission to make the request, in either of the following cases: when the object has no prior retention mode set or if a previous retention policy has already expired when the request extends the retention period later than the existing retention deadline if the object has GOVERNANCE mode set and the request includes the x-amz-bypass-governance-retention:true header Note: Delete markers do not have any object lock protection Processing DELETE requests Whenever requests such as DELETE object using version-id, Multi-Object DELETE specifying version-id or a Lifecycle action to permanently delete the object version are received, the current date and time is compared against the retain-until-date set on the object and the client will receive an Access Denied error if the current date and time is less than the retain-until-date set on the object. When the current date and time exceeds the retain-until-date , deletes on the object version are not prevented by the object lock mechanism. There is no cleanup action to remove the retain-until-date set on the object version's metadata once the retain-until-date expires. DELETE object requests without a version id result in creation of delete markers on top of the object version, even if the object has a lock configuration set and the retain-until-date is current. Lifecycle actions Lifecycle jobs can create delete markers on the object but cannot permanently delete an object version that is locked until the lock expires. Replication The lock configuration on the object version in a source bucket is copied over to the destination bucket only if object lock is enabled on the destination bucket. Otherwise, the lock is ignored in the destination bucket. APIs covering S3 Object lock Put Bucket - extend the bucket creation API to include configuration for enabling object lock on the bucket Note: Versioning is automatically enabled on buckets that have object lock enabled as part of a PUT Bucket request Put Object - extend put object API to parse x-amz-object-lock-mode , x-amz-object-lock-retain-until-date , x-amz-object-lock-legal-hold headers and store the configuration on an object. Copy Object - extend the API to accept the same lock configuration headers as the PUT Object request Create Multipart Upload - extend the API to accept the same lock configuration headers as the PUT Object request Put Object Lock Configuration - allows setting a default lock configuration for objects that are going to be stored in the bucket. This request is accepted only on buckets that have object lock enabled. Get Object Lock Configuration - gets the object lock configuration set on the bucket metadata Put Object Retention - sets the retention mode/period configuration on a object version Get Object Retention - gets the retention mode/period configuration set on a object version Put Object Legal Hold - sets legal hold configuration on an object version Get Object Legal Hold - gets the legal hold status for an object version Deviations The following hightlights any deviation from AWS implementation of the feature token AWS allows setting object lock on existing buckets by asking the user to contact support, the support then gives a token that can be used to set lock on the specific bucket. Cloudserver's implementation differs in the way that we will use a tool to enable object lock by modifying the bucket's configuration Migration tool Migration tool will be available to enable lock on an existing bucket. This tool will be available in s3utils to be used at a customer. FAQ Q: What error code does the client get when a delete version request is sent on a locked object? A: Access Denied Q: Can an object set with GOVERNANCE retention mode be deleted with account credentials? A: Yes, if the delete object request is sent is along with x-amz-bypass-governance-retention:true header. This header needs to be explicitly set and is disabled by default on all AWS SDKs and AWS CLI. Q: Does placing a lock on an object version generate a new version? A: No. It updates the object version with retention settings without generating a new version-id or updating the Last-Modified date on the object version.","title":"Object Lock"},{"location":"design/object-lock/#object-locking-using-s3-object-lock","text":"Object locking is a feature built for addressing use cases in which the Write Once Read Many (WORM) model is required. The feature will be built in line with the AWS specification. Any extensions to the specification will be explicitly documented. The feature implementation has the end goal of meeting SEC 17a-4 compliance.","title":"Object Locking using S3 Object Lock"},{"location":"design/object-lock/#requirements","text":"Object lock flag must be set during bucket creation Versioning has to be enabled on the bucket (it is enabled automatically at creation time when object lock flag is set) Object lock must be enabled on a bucket in order to write a lock configuration using the PUT Object Lock Configuration api that has object lock flag set PS: AWS S3 specification does not have a way of setting object lock on existing buckets. To enable object lock flag on an existing bucket, a tool provided by Scality can be used see migration tool section . Objects that were created before the lock is set are not protected by Object Locking.","title":"Requirements"},{"location":"design/object-lock/#what-happens-when-an-object-is-locked","text":"When set on an object version, a lock prevents deletes and overwrites on that version until the lock expires. However, the lock doesn't prevent creation of delete markers or new versions on top of the locked version. Object locking also protects objects from Lifecycle actions, i.e a Lifecycle expiry rule cannot delete an object version until the lock on the object expires.","title":"What happens when an object is locked"},{"location":"design/object-lock/#controlling-locking-of-an-object","text":"S3 provides a few ways through which the lock configuration of an object can be set","title":"Controlling locking of an object"},{"location":"design/object-lock/#retention-modes","text":"Both Governance and Compliance modes retain the lock on an object until the set retention period expires.","title":"Retention Modes"},{"location":"design/object-lock/#governance-mode","text":"In addition to preventing deletions on an object version by users, GOVERNANCE mode allows delegating permission to certain users to override the lock settings, for example by changing the retention mode, period or deleting the object altogether. Either the root (account) or a user with s3:BypassGovernanceRetention permission can send a delete request with x-amz-bypass-governance-retention:true header to override and delete the object.","title":"Governance mode"},{"location":"design/object-lock/#compliance-mode","text":"When a lock is placed on an object using COMPLIANCE mode, the object version cannot be deleted by any user until the retention period expires. This includes root user (account credentials) in the account and no other user can be given permission either to override the settings or delete the version.","title":"Compliance mode"},{"location":"design/object-lock/#retention-period","text":"Retention period defines the term during which the object is protected from deletes. Retention period can be set on the bucket level, which acts as a default for all objects put in the bucket after the setting is applied. The default retention period set on a bucket can be overridden on the object level by setting the date and time using the header x-amz-object-lock-retain-until-date . Buckets - retention period can be set in either days or years , but not both at the same time. Objects - retention period can be set as date and time when the object is expected to expire.","title":"Retention Period"},{"location":"design/object-lock/#legal-hold","text":"Legal hold can be enabled on an object version. Once a legal hold is enabled, regardless of the object's retention date or retention mode, the object version cannot be deleted until the legal hold is removed. Legal hold can be set on an object version during PUT Object request by setting x-amz-object-lock-legal-hold header or using PUT Object Legal Hold API request. Root users with account credentials or IAM users who are given the permission s3:PutObjectLegalHold are allowed to set Legal hold on an object version.","title":"Legal Hold"},{"location":"design/object-lock/#implementation","text":"","title":"Implementation"},{"location":"design/object-lock/#storing-object-lock-configuration","text":"Object lock can be enabled on a bucket during bucket creation using x-amz-bucket-object-lock-enabled header. This is stored along with the bucket's metadata defined in the BucketInfo model. Any default lock settings (governance/compliance mode, days/years for retention period) defined using PutObjectLockConfiguration request are stored as part of the bucket metadata. When a PUT Object request is received, the lock configuration is evaluated and the lock expiration date and time is calculated and set as retain-until-date property on the object's metadata. Lock configuration on a PUT object request is evaluated in the following order: Object's configuration in the PUT Object for retention mode/period Bucket's configuration for retention mode/period The object's settings override the bucket's settings when calculating the retain-until-date date and time to be stored on the object's metadata. The lock configuration on an object version can also be changed using PutObjectRetention api request if the user has appropriate permission to make the request, in either of the following cases: when the object has no prior retention mode set or if a previous retention policy has already expired when the request extends the retention period later than the existing retention deadline if the object has GOVERNANCE mode set and the request includes the x-amz-bypass-governance-retention:true header Note: Delete markers do not have any object lock protection","title":"Storing Object lock configuration"},{"location":"design/object-lock/#processing-delete-requests","text":"Whenever requests such as DELETE object using version-id, Multi-Object DELETE specifying version-id or a Lifecycle action to permanently delete the object version are received, the current date and time is compared against the retain-until-date set on the object and the client will receive an Access Denied error if the current date and time is less than the retain-until-date set on the object. When the current date and time exceeds the retain-until-date , deletes on the object version are not prevented by the object lock mechanism. There is no cleanup action to remove the retain-until-date set on the object version's metadata once the retain-until-date expires. DELETE object requests without a version id result in creation of delete markers on top of the object version, even if the object has a lock configuration set and the retain-until-date is current.","title":"Processing DELETE requests"},{"location":"design/object-lock/#lifecycle-actions","text":"Lifecycle jobs can create delete markers on the object but cannot permanently delete an object version that is locked until the lock expires.","title":"Lifecycle actions"},{"location":"design/object-lock/#replication","text":"The lock configuration on the object version in a source bucket is copied over to the destination bucket only if object lock is enabled on the destination bucket. Otherwise, the lock is ignored in the destination bucket.","title":"Replication"},{"location":"design/object-lock/#apis-covering-s3-object-lock","text":"Put Bucket - extend the bucket creation API to include configuration for enabling object lock on the bucket Note: Versioning is automatically enabled on buckets that have object lock enabled as part of a PUT Bucket request Put Object - extend put object API to parse x-amz-object-lock-mode , x-amz-object-lock-retain-until-date , x-amz-object-lock-legal-hold headers and store the configuration on an object. Copy Object - extend the API to accept the same lock configuration headers as the PUT Object request Create Multipart Upload - extend the API to accept the same lock configuration headers as the PUT Object request Put Object Lock Configuration - allows setting a default lock configuration for objects that are going to be stored in the bucket. This request is accepted only on buckets that have object lock enabled. Get Object Lock Configuration - gets the object lock configuration set on the bucket metadata Put Object Retention - sets the retention mode/period configuration on a object version Get Object Retention - gets the retention mode/period configuration set on a object version Put Object Legal Hold - sets legal hold configuration on an object version Get Object Legal Hold - gets the legal hold status for an object version","title":"APIs covering S3 Object lock"},{"location":"design/object-lock/#deviations","text":"The following hightlights any deviation from AWS implementation of the feature token AWS allows setting object lock on existing buckets by asking the user to contact support, the support then gives a token that can be used to set lock on the specific bucket. Cloudserver's implementation differs in the way that we will use a tool to enable object lock by modifying the bucket's configuration","title":"Deviations"},{"location":"design/object-lock/#migration-tool","text":"Migration tool will be available to enable lock on an existing bucket. This tool will be available in s3utils to be used at a customer.","title":"Migration tool"},{"location":"design/object-lock/#faq","text":"Q: What error code does the client get when a delete version request is sent on a locked object? A: Access Denied Q: Can an object set with GOVERNANCE retention mode be deleted with account credentials? A: Yes, if the delete object request is sent is along with x-amz-bypass-governance-retention:true header. This header needs to be explicitly set and is disabled by default on all AWS SDKs and AWS CLI. Q: Does placing a lock on an object version generate a new version? A: No. It updates the object version with retention settings without generating a new version-id or updating the Last-Modified date on the object version.","title":"FAQ"},{"location":"design/utapi/","text":"Utapi - Utilization API Overview Utapi tracks metrics of a service's usage. Metrics provided by Utapi include the number of incoming and outgoing bytes, the number of objects being stored, the storage utilized in bytes, and a count of operations performed on a service's resources. Operations supported by Utapi include APIs offered by Scality's S3 Server. Metrics can be retrieved for a given time range in a service's history. Problem Description Utapi needs to provide a scalable solution to accurately track metrics across any number of nodes while providing reliable storage of its historic data. The system should be highly available to avoid losing data and affecting the quality of the provided metrics. It should be able to tolerate node and service failures and be able to repair it's data in the event of a failure. Technical Details Ingestion Flow +--------------------------------+ | Local Redis | +--------------------------------+ ^ | | | | v +-------------+ +------------+ +---------------+ +-------+ | Cloudserver | +-----> |Utapi Server| |Background Task| +-----> |Warp 10| +-------------+ +------------+ +---------------+ +-------+ Data Model and Operations Redis Utapi uses Redis to cache events before inserting them into Warp 10. This is to reduce write overhead as batch inserts are more efficient. Each event is JSON serialized and stored in a key. Based on its timestamp the event is then sharded into a 10 second block and its key is added to a corresponding Set. Key Schema Events are stored using <prefix>:events:<event uuid> Shards are stored using <prefix>:shards:<timestamp> timestamp is a UNIX style timestamp indicating the beginning of the shard Warp 10 Warp 10 is used for long term storage and analysis of events in Utapi. In Warp 10 parlance each event is a measure consisting of a timestamp, class , key value pair labels , and an associated value. Utapi defines several classes: utapi.events used for ingested events utapi.checkpoints used for incremental checkpoints of events utapi.checkpoints.master used to record checkpoint creation utapi.snapshots used for incremental snapshots of checkpoints utapi.snapshots.master used to record snapshot creation utapi.repairs utapi.repairs.master High level overview Snapshots Checkpoints Events +-----------+ +-----------+ +-------------+ +---------+ | -1 | +---------+ +---------+ | -1 | +---------+ +-------+ <------+ +---------+ | -3 | | | -1 | +-------+ | +---------+ +---+ +---------+ | | -1 | | +---------+ +---+ +---------+ | -1 | +---------+ +-------+ <------+ +-------+ <------+ +---------+ | 5 | | | -1 | | | -1 | +-------+ | +-------+ | +---------+ | +---+ +---------+ | | | -1 | | | +---------+ | +---+ +---------+ | | +1 | | +---------+ +---+ +-------+ <------+ +---------+ | | +3 | | | +1 | | +-------+ | +---------+ | +---+ +---------+ | | | +1 | | | +---------+ | +---+ +---------+ | | +1 | | +---------+ +---+ +-------+ <------+ +---------+ | +3 | | | +1 | +-------+ | +---------+ +---+ +---------+ | | +1 | | +---------+ +---+ +---------+ | +1 | +---------+ Events Utapi stores four integers, using Warp 10's multivariate data type, for every operation it ingests, objectDelta , bytesDelta , ingress , and egress . They reflect how an operation affects the computed metric counters and can be both positive and negative. For example, an upload of a 100 byte object to a new key would have the values: { \"objectDelta\" : 1 , \"bytesDelta\" : 100 , \"ingress\" : 100 , \"egress\" : 0 } Deleting the same object would have { \"objectDelta\" : -1 , \"bytesDelta\" : -100 , \"ingress\" : 0 , \"egress\" : 0 } Each event can also have attached metadata such as the account , bucket , or location of the affected resource. These are attached as Warp 10 labels to the event and are used during checkpoint creation to create the various levels of metrics. Checkpoints Checkpoints are the first step in converting the stream of events into a form we can use to compute metrics. They are created using a timestamp and a list of label names to index. Each provided label creates a \"level\" of metrics and a checkpoint will be created for each unique label name/value pair encountered. Starting at the end timestamp a previously created master checkpoint is search for, its timestamp will be used for the start of the checkpointed range. If no previous master checkpoint is found a timestamp of 0 is used, causing all events to be scanned. Events during the calculated time range are retrieved and iterated over. If any of the specified labels are seen the event's values are taken and summed with any previous values matching the same label value. A count of each value of the label operationId is also kept for every checkpoint. For example, the following events (shown in GTS format for brevity) if used with the label bucket will produce two checkpoints tagged with bucket0 and bucket1 respectively. 1000000000000000 utapi.events{operatioinId=putObject bucket=bucket0 key=obj0} [ 1 100 100 0 ] 1000000000000001 utapi.events{operatioinId=putObject bucket=bucket0 key=obj1} [ 1 100 100 0 ] 1000000000000002 utapi.events{operatioinId=putObject bucket=bucket1 key=obj0} [ 1 100 100 0 ] 1000000000000003 utapi.events{operatioinId=putObject bucket=bucket1 key=obj2} [ 1 100 100 0 ] 1000000000000003 utapi.events{operatioinId=deleteObject bucket=bucket1 key=obj2} [ -1 -100 0 0 ] 1000000000000010 utapi.checkpoints{bucket=bucket0} [ 2 200 200 0 { \"ops\": { \"putObject\": 2 } } ] 1000000000000010 utapi.checkpoints{bucket=bucket1} [ 1 100 200 0 { \"ops\": { \"putObject\": 2, \"deleteObject\": 1 } } ] After the events are scanned each checkpoint is stored as a measure in Warp 10 using the class utapi.checkpoints . A single label is attached with the label name and value being the level and group of the checkpoint (ie bucket and bucket0 ) from the example above. This allows us to easily query for a specific level's checkpoints. Once all checkpoints have been stored a \"master\" checkpoint is created with the class utapi.checkpoints.master The passed end timestamp is used for the master checkpoint along with any created checkpoints to signify the end of the scanned range. Snapshots Snapshots are used to convert the delta style checkpoints into concrete numbers and provide a starting point for calculating the final metrics rather than processing every checkpoint. Snapshots are created using only an end timestamp signifying the end of the time range of included checkpoints. Like checkpoint creation the latest master snapshot is searched for and all checkpoints with timestamp lying within the master snapshots timestamp and the passed timestamp are fetched. If no previous master snapshots are found a timestamp of 0 is used causing all checkpoints to be included. The checkpoints are then iterated over and summed together based upon their labels using the values from the previous snapshot as a base. For example the two checkpoints from the previous example: 1000000000000010 utapi.checkpoints{bucket=bucket0} [ 2 200 200 0 { \"ops\": { \"putObject\": 2 } } ] 1000000000000010 utapi.checkpoints{bucket=bucket1} [ 1 100 200 0 { \"ops\": { \"putObject\": 2, \"deleteObject\": 1 } } ] Will produce these two snapshots: 1000000000000010 utapi.snapshots{bucket=bucket0} [ 2 200 200 0 { \"ops\": { \"putObject\": 2 } } ] 1000000000000010 utapi.snapshots{bucket=bucket1} [ 1 100 200 0 { \"ops\": { \"putObject\": 2, \"deleteObject\": 1 } } ] Repairs In a distributed system node or network failures can cause events to not be immediately ingested into Utapi and can cause problems for a system rely on a static ordering of historic events. To guard against this Utapi implements a system for asynchronous repairs of checkpoints and snapshots. During the Redis -> Warp 10 transition Utapi performs some sanity checks to determine if the inserted metrics could have already included in a checkpoint. If so the repair process is started using the beginning and end of the time range of the inserted events as the beginning and end of the repair range. Effected checkpoints are fetched and recalculated to include the new events. Snapshots including the updated checkpoints are then fetched and recalculated. +-------+ <--+---+ +-------+ <--+---+ +---------+ | 3 | | | -1 | | | -1 | +-------+ | +-------+ | +---------+ ^ | +---+ +---------+ | | | | -1 | Snapshot | | +---------+ updated | +---+ +---------+ | | +1 | | +---------+ +---+ +-------+ <--+---+ +---------+ | | +1 | | | +1 | Checkpoint +--------> +-------+ | +---------+ is updated | +---+ +---------+ | | | +1 | | | +---------+ | |---+ +---------+ | | | -1 | <----+ New events are inserted | | +---------+ | | |---+ +---------+ | | | | -1 | <----+ | | +---------+ | +---+ +---------+ | | +1 | | +---------+ +---+ +-------+ <--+---+ +---------+ | +3 | | | +1 | +-------+ | +---------+ +---+ +---------+ | | +1 | | +---------+ +---+ +---------+ | +1 | +---------+ Failure Recovery Failure scenarios are mitigated using caching with retries at various levels of the system with specially formatted logs messages being used as a last resort. Below are some specific failure scenarios that have been considered. Local Redis Unavailable In the case of the local redis being unavailable Cloudserver will cache events in memory up to a limit and then will begin flushing events as special log messages. After redis is available, events will be flushed and a repair triggered if needed. Warp 10 Unavailable If Warp 10 is unavailable events are persisted in redis until it becomes available and then batch inserted. Migrations Migrations from previous customer deployments will require a post upgrade tool to be used. This will migrate the historic utapi data in Redis into Warp 10 and make it available under the new system..","title":"UTAPI"},{"location":"design/utapi/#utapi-utilization-api","text":"","title":"Utapi - Utilization API"},{"location":"design/utapi/#overview","text":"Utapi tracks metrics of a service's usage. Metrics provided by Utapi include the number of incoming and outgoing bytes, the number of objects being stored, the storage utilized in bytes, and a count of operations performed on a service's resources. Operations supported by Utapi include APIs offered by Scality's S3 Server. Metrics can be retrieved for a given time range in a service's history.","title":"Overview"},{"location":"design/utapi/#problem-description","text":"Utapi needs to provide a scalable solution to accurately track metrics across any number of nodes while providing reliable storage of its historic data. The system should be highly available to avoid losing data and affecting the quality of the provided metrics. It should be able to tolerate node and service failures and be able to repair it's data in the event of a failure.","title":"Problem Description"},{"location":"design/utapi/#technical-details","text":"","title":"Technical Details"},{"location":"design/utapi/#ingestion-flow","text":"+--------------------------------+ | Local Redis | +--------------------------------+ ^ | | | | v +-------------+ +------------+ +---------------+ +-------+ | Cloudserver | +-----> |Utapi Server| |Background Task| +-----> |Warp 10| +-------------+ +------------+ +---------------+ +-------+","title":"Ingestion Flow"},{"location":"design/utapi/#data-model-and-operations","text":"","title":"Data Model and Operations"},{"location":"design/utapi/#redis","text":"Utapi uses Redis to cache events before inserting them into Warp 10. This is to reduce write overhead as batch inserts are more efficient. Each event is JSON serialized and stored in a key. Based on its timestamp the event is then sharded into a 10 second block and its key is added to a corresponding Set. Key Schema Events are stored using <prefix>:events:<event uuid> Shards are stored using <prefix>:shards:<timestamp> timestamp is a UNIX style timestamp indicating the beginning of the shard","title":"Redis"},{"location":"design/utapi/#warp-10","text":"Warp 10 is used for long term storage and analysis of events in Utapi. In Warp 10 parlance each event is a measure consisting of a timestamp, class , key value pair labels , and an associated value. Utapi defines several classes: utapi.events used for ingested events utapi.checkpoints used for incremental checkpoints of events utapi.checkpoints.master used to record checkpoint creation utapi.snapshots used for incremental snapshots of checkpoints utapi.snapshots.master used to record snapshot creation utapi.repairs utapi.repairs.master High level overview Snapshots Checkpoints Events +-----------+ +-----------+ +-------------+ +---------+ | -1 | +---------+ +---------+ | -1 | +---------+ +-------+ <------+ +---------+ | -3 | | | -1 | +-------+ | +---------+ +---+ +---------+ | | -1 | | +---------+ +---+ +---------+ | -1 | +---------+ +-------+ <------+ +-------+ <------+ +---------+ | 5 | | | -1 | | | -1 | +-------+ | +-------+ | +---------+ | +---+ +---------+ | | | -1 | | | +---------+ | +---+ +---------+ | | +1 | | +---------+ +---+ +-------+ <------+ +---------+ | | +3 | | | +1 | | +-------+ | +---------+ | +---+ +---------+ | | | +1 | | | +---------+ | +---+ +---------+ | | +1 | | +---------+ +---+ +-------+ <------+ +---------+ | +3 | | | +1 | +-------+ | +---------+ +---+ +---------+ | | +1 | | +---------+ +---+ +---------+ | +1 | +---------+","title":"Warp 10"},{"location":"design/utapi/#events","text":"Utapi stores four integers, using Warp 10's multivariate data type, for every operation it ingests, objectDelta , bytesDelta , ingress , and egress . They reflect how an operation affects the computed metric counters and can be both positive and negative. For example, an upload of a 100 byte object to a new key would have the values: { \"objectDelta\" : 1 , \"bytesDelta\" : 100 , \"ingress\" : 100 , \"egress\" : 0 } Deleting the same object would have { \"objectDelta\" : -1 , \"bytesDelta\" : -100 , \"ingress\" : 0 , \"egress\" : 0 } Each event can also have attached metadata such as the account , bucket , or location of the affected resource. These are attached as Warp 10 labels to the event and are used during checkpoint creation to create the various levels of metrics.","title":"Events"},{"location":"design/utapi/#checkpoints","text":"Checkpoints are the first step in converting the stream of events into a form we can use to compute metrics. They are created using a timestamp and a list of label names to index. Each provided label creates a \"level\" of metrics and a checkpoint will be created for each unique label name/value pair encountered. Starting at the end timestamp a previously created master checkpoint is search for, its timestamp will be used for the start of the checkpointed range. If no previous master checkpoint is found a timestamp of 0 is used, causing all events to be scanned. Events during the calculated time range are retrieved and iterated over. If any of the specified labels are seen the event's values are taken and summed with any previous values matching the same label value. A count of each value of the label operationId is also kept for every checkpoint. For example, the following events (shown in GTS format for brevity) if used with the label bucket will produce two checkpoints tagged with bucket0 and bucket1 respectively. 1000000000000000 utapi.events{operatioinId=putObject bucket=bucket0 key=obj0} [ 1 100 100 0 ] 1000000000000001 utapi.events{operatioinId=putObject bucket=bucket0 key=obj1} [ 1 100 100 0 ] 1000000000000002 utapi.events{operatioinId=putObject bucket=bucket1 key=obj0} [ 1 100 100 0 ] 1000000000000003 utapi.events{operatioinId=putObject bucket=bucket1 key=obj2} [ 1 100 100 0 ] 1000000000000003 utapi.events{operatioinId=deleteObject bucket=bucket1 key=obj2} [ -1 -100 0 0 ] 1000000000000010 utapi.checkpoints{bucket=bucket0} [ 2 200 200 0 { \"ops\": { \"putObject\": 2 } } ] 1000000000000010 utapi.checkpoints{bucket=bucket1} [ 1 100 200 0 { \"ops\": { \"putObject\": 2, \"deleteObject\": 1 } } ] After the events are scanned each checkpoint is stored as a measure in Warp 10 using the class utapi.checkpoints . A single label is attached with the label name and value being the level and group of the checkpoint (ie bucket and bucket0 ) from the example above. This allows us to easily query for a specific level's checkpoints. Once all checkpoints have been stored a \"master\" checkpoint is created with the class utapi.checkpoints.master The passed end timestamp is used for the master checkpoint along with any created checkpoints to signify the end of the scanned range.","title":"Checkpoints"},{"location":"design/utapi/#snapshots","text":"Snapshots are used to convert the delta style checkpoints into concrete numbers and provide a starting point for calculating the final metrics rather than processing every checkpoint. Snapshots are created using only an end timestamp signifying the end of the time range of included checkpoints. Like checkpoint creation the latest master snapshot is searched for and all checkpoints with timestamp lying within the master snapshots timestamp and the passed timestamp are fetched. If no previous master snapshots are found a timestamp of 0 is used causing all checkpoints to be included. The checkpoints are then iterated over and summed together based upon their labels using the values from the previous snapshot as a base. For example the two checkpoints from the previous example: 1000000000000010 utapi.checkpoints{bucket=bucket0} [ 2 200 200 0 { \"ops\": { \"putObject\": 2 } } ] 1000000000000010 utapi.checkpoints{bucket=bucket1} [ 1 100 200 0 { \"ops\": { \"putObject\": 2, \"deleteObject\": 1 } } ] Will produce these two snapshots: 1000000000000010 utapi.snapshots{bucket=bucket0} [ 2 200 200 0 { \"ops\": { \"putObject\": 2 } } ] 1000000000000010 utapi.snapshots{bucket=bucket1} [ 1 100 200 0 { \"ops\": { \"putObject\": 2, \"deleteObject\": 1 } } ]","title":"Snapshots"},{"location":"design/utapi/#repairs","text":"In a distributed system node or network failures can cause events to not be immediately ingested into Utapi and can cause problems for a system rely on a static ordering of historic events. To guard against this Utapi implements a system for asynchronous repairs of checkpoints and snapshots. During the Redis -> Warp 10 transition Utapi performs some sanity checks to determine if the inserted metrics could have already included in a checkpoint. If so the repair process is started using the beginning and end of the time range of the inserted events as the beginning and end of the repair range. Effected checkpoints are fetched and recalculated to include the new events. Snapshots including the updated checkpoints are then fetched and recalculated. +-------+ <--+---+ +-------+ <--+---+ +---------+ | 3 | | | -1 | | | -1 | +-------+ | +-------+ | +---------+ ^ | +---+ +---------+ | | | | -1 | Snapshot | | +---------+ updated | +---+ +---------+ | | +1 | | +---------+ +---+ +-------+ <--+---+ +---------+ | | +1 | | | +1 | Checkpoint +--------> +-------+ | +---------+ is updated | +---+ +---------+ | | | +1 | | | +---------+ | |---+ +---------+ | | | -1 | <----+ New events are inserted | | +---------+ | | |---+ +---------+ | | | | -1 | <----+ | | +---------+ | +---+ +---------+ | | +1 | | +---------+ +---+ +-------+ <--+---+ +---------+ | +3 | | | +1 | +-------+ | +---------+ +---+ +---------+ | | +1 | | +---------+ +---+ +---------+ | +1 | +---------+","title":"Repairs"},{"location":"design/utapi/#failure-recovery","text":"Failure scenarios are mitigated using caching with retries at various levels of the system with specially formatted logs messages being used as a last resort. Below are some specific failure scenarios that have been considered. Local Redis Unavailable In the case of the local redis being unavailable Cloudserver will cache events in memory up to a limit and then will begin flushing events as special log messages. After redis is available, events will be flushed and a repair triggered if needed. Warp 10 Unavailable If Warp 10 is unavailable events are persisted in redis until it becomes available and then batch inserted.","title":"Failure Recovery"},{"location":"design/utapi/#migrations","text":"Migrations from previous customer deployments will require a post upgrade tool to be used. This will migrate the historic utapi data in Redis into Warp 10 and make it available under the new system..","title":"Migrations"},{"location":"release/","text":"Object Squad Release Process This is the official documentation for how the Object Squad will handle releases. We expect this release process will: Give us a simpler, more conventional release workflow. Speed our development, improving code-complete-to-GA time and end-to-end test feedback. Enable us to release more and smaller. Untangle our projects and enable us to manage them independently. Reduce the stress around releases.","title":"General"},{"location":"release/#object-squad-release-process","text":"This is the official documentation for how the Object Squad will handle releases. We expect this release process will: Give us a simpler, more conventional release workflow. Speed our development, improving code-complete-to-GA time and end-to-end test feedback. Enable us to release more and smaller. Untangle our projects and enable us to manage them independently. Reduce the stress around releases.","title":"Object Squad Release Process"},{"location":"release/disconnecting/","text":"Disconnecting Releases We are currently in a multi-repository environment. We must redefine how our products are connected to their component repositories, taking how we intend to release products into consideration. Component Repositories We have many libraries, APIs, and tools, which we have coded in-house. For example, Arsenal, CloudServer, and Backbeat. Let's treat them as we would any open-source component we use in a project. Each component must contain: Unit and functional tests to ensure that the component works. Documentation explaining the services the component provides, and how to contribute, build, test, configure, run, use, and (potentially) monitor the component. Release notes Build chain for building, testing, and publishing packages Each component will thus have its own release cycle that is independent of the release cycle of the product itself. The number of component releases, therefore, does not affect the product release cycle. Selecting a component should be as easy and understandable as picking one from the open source world either from it\u2019s proper language registry (like, for example, bumping the version on the package.json file for Node.js projects) or pulling a Docker image from a registry. The component must therefore make those packages or images available. Any member of the object squad can release a component. They can ask the CI chapter for help publicly or privately if it's needed. Release notes are provided with each release. Project Repositories The purpose of a project repository is to: Select components (developed in-house or from open source projects) by version. Create a build chain that outputs the desired installation code and packages. Trigger integration tests that ensure all selected components interoperate effectively. Document the project's services, and how to build, test, configure, run, monitor, and contribute to the project. Offer release notes that document changes in the product's function and features. Project releases should be kept as small as possible. For example, bumping the version of a component might unlock features or bugfixes. If we keep shipping releases, we can advance customers a few at a time and receive needed feedback along the way. The responsibility for providing instructions and all required packages for installation is at the project level, not the product level. Release readiness is determined by the project owner. Product Repositories Product repositories contain the code and documentation that are shipped to the customer. The purpose of a product repository is to: Assemble projects and components, maintaining their respective version numbers. Create a build chain that outputs the desired installation code and packages. Trigger integration tests that ensure all selected projects interoperate effectively. Document the product's services; how to install, configure, run, and monitor the product; and how to solve product issues. Offer release notes that document changes in the project's function and features. Product releases are driven by customer needs and are not defined within the object squad. The object squad, however, is responsible for making project releases available to the product and for helping integrate the projects into the product.","title":"Disconnecting"},{"location":"release/disconnecting/#disconnecting-releases","text":"We are currently in a multi-repository environment. We must redefine how our products are connected to their component repositories, taking how we intend to release products into consideration.","title":"Disconnecting Releases"},{"location":"release/disconnecting/#component-repositories","text":"We have many libraries, APIs, and tools, which we have coded in-house. For example, Arsenal, CloudServer, and Backbeat. Let's treat them as we would any open-source component we use in a project. Each component must contain: Unit and functional tests to ensure that the component works. Documentation explaining the services the component provides, and how to contribute, build, test, configure, run, use, and (potentially) monitor the component. Release notes Build chain for building, testing, and publishing packages Each component will thus have its own release cycle that is independent of the release cycle of the product itself. The number of component releases, therefore, does not affect the product release cycle. Selecting a component should be as easy and understandable as picking one from the open source world either from it\u2019s proper language registry (like, for example, bumping the version on the package.json file for Node.js projects) or pulling a Docker image from a registry. The component must therefore make those packages or images available. Any member of the object squad can release a component. They can ask the CI chapter for help publicly or privately if it's needed. Release notes are provided with each release.","title":"Component Repositories"},{"location":"release/disconnecting/#project-repositories","text":"The purpose of a project repository is to: Select components (developed in-house or from open source projects) by version. Create a build chain that outputs the desired installation code and packages. Trigger integration tests that ensure all selected components interoperate effectively. Document the project's services, and how to build, test, configure, run, monitor, and contribute to the project. Offer release notes that document changes in the product's function and features. Project releases should be kept as small as possible. For example, bumping the version of a component might unlock features or bugfixes. If we keep shipping releases, we can advance customers a few at a time and receive needed feedback along the way. The responsibility for providing instructions and all required packages for installation is at the project level, not the product level. Release readiness is determined by the project owner.","title":"Project Repositories"},{"location":"release/disconnecting/#product-repositories","text":"Product repositories contain the code and documentation that are shipped to the customer. The purpose of a product repository is to: Assemble projects and components, maintaining their respective version numbers. Create a build chain that outputs the desired installation code and packages. Trigger integration tests that ensure all selected projects interoperate effectively. Document the product's services; how to install, configure, run, and monitor the product; and how to solve product issues. Offer release notes that document changes in the project's function and features. Product releases are driven by customer needs and are not defined within the object squad. The object squad, however, is responsible for making project releases available to the product and for helping integrate the projects into the product.","title":"Product Repositories"},{"location":"requirements/s3-replication-monitoring/","text":"S3 Replication Monitoring This document describes S3 Replication monitoring requirements. Context S3 replication enables asynchronous object replication between a source and a destination bucket. These buckets can be located in the same region or in different regions. The Storage Administrator and the Storage Account Owner roles monitor the replication service from two points of view (PoV): S3 replication service provider PoV S3 replication service client PoV Storage Administrator Monitoring Storage Administrators are responsible for operating the Scality platform. They monitor the status and performance of the components that deliver the replication service. Storage Account Owner Monitoring The Storage account owner is responsible for configuring replication rules for an S3 client application, monitoring the replication service to make sure a replication rule meets the application requirements. S3 Replication Components Definition The S3 replication components are: Queue Populator Queue Processor Status Processor Kafka brokers (third-party) Kafka topics (third-party) Kafka partitions (third-party) Zookeeper quorum (third-party) Storage Administrator User Stories As a Storage Administrator I want a Monitoring Dashboard for S3 replication components So that I can identify configuration, performance, or network issues Global Acceptance Criteria API S3 replication components metrics must be exposed through HTTP endpoint following Prometheus standards UI Third-party components dashboard must have an open-source monitoring dashboard S3 monitoring graphs should not show S3 replication operations S3 replication service must have a global monitoring dashboard containing: Current number of healthy vs unhealthy S3 replication components Number of objects replicated in the destinations over time Number of bytes replicated in the destinations over time Number of objects in the replication backlog over time Number of bytes in the replication backlog over time Alerting The product must allow to seed default alerting rules related to replication metrics. Deployment Replication monitoring components and monitoring dashboards must be deployed automatically in the product. Documentation Product documentation must describe: Network flows required to monitor S3 replication components (source, port destination, and protocol used) S3 replication monitoring configuration steps S3 replication components metrics S3 replication monitoring dashboards Alerting definition Queue Populator Acceptance Criteria Queue Populator Monitoring API The Queue Populator must expose the following metrics: State (running, initializing, or failed) Raft session metadata journal read offset Raft session metadata journal size Number of metadata journal entries processed since startup Number of Kafka messages produced on the topic since startup The Queue Populator metrics must be filterable by: Raft session ID Host name Docker container name Queue Populator Monitoring UI The Queue Populator must have the following monitoring graphs: Number of Queue Processor State Metadata journal lag Metadata journal entry processed Messaged queued Queue Processor Acceptance Criteria Queue Processor Monitoring API The Queue Processor must expose the following metrics: State (running, idle, initializing, or failed) Kafka consumer offset in the Kafka partitions Offset of the last message produced in the Kafka partitions Number of messages consumed in the Kafka partitions since startup Operations generated to retrieve (GET) objects from the source since startup: Number of operation Status Average latency Total bandwidth consumed Operations generated to write (PUT) object data and metadata in the destination since startup: Number of operation Status Average latency Total bandwidth consumed Queue Processor metrics must be filterable by: Kafka partition ID Host name Docker container name Queue Processor Monitoring UI The Queue Processor must have the following monitoring graphs: Number of Queue Processor State Partition lag Operations generated to retrieve (GET) objects on the source Number of operation Operation status (success, retry, failed) Average latency Total bandwidth consumed Operations generated to write (PUT) object data and metadata in the destination Number of operation Operation status (success, retry, failed) Average latency Total bandwidth consumed Status Processor Acceptance Criteria Status Processor Monitoring API The Status Processor must exposed the following metrics: State (running, idle, initializing, or failed) Kafka consumer offset in the Kafka partitions Operations generated to update objects metadata in the source since startup Number of operation Status Average latency Total bandwidth consumed The Status Processor metrics must be filtrable by: Kafka partition ID Host Name Docker container name Status Processor Monitoring UI The Status Processor must have the following monitoring graphs: Number of Status Processor State Partitions lag Operations generated to update objects metadata in the source Number of operation Operation status (success, retry, failed) Average latency Total bandwidth consumed","title":"S3 Replication Monitoring"},{"location":"requirements/s3-replication-monitoring/#s3-replication-monitoring","text":"This document describes S3 Replication monitoring requirements.","title":"S3 Replication Monitoring"},{"location":"requirements/s3-replication-monitoring/#context","text":"S3 replication enables asynchronous object replication between a source and a destination bucket. These buckets can be located in the same region or in different regions. The Storage Administrator and the Storage Account Owner roles monitor the replication service from two points of view (PoV): S3 replication service provider PoV S3 replication service client PoV","title":"Context"},{"location":"requirements/s3-replication-monitoring/#storage-administrator-monitoring","text":"Storage Administrators are responsible for operating the Scality platform. They monitor the status and performance of the components that deliver the replication service.","title":"Storage Administrator Monitoring"},{"location":"requirements/s3-replication-monitoring/#storage-account-owner-monitoring","text":"The Storage account owner is responsible for configuring replication rules for an S3 client application, monitoring the replication service to make sure a replication rule meets the application requirements.","title":"Storage Account Owner Monitoring"},{"location":"requirements/s3-replication-monitoring/#s3-replication-components-definition","text":"The S3 replication components are: Queue Populator Queue Processor Status Processor Kafka brokers (third-party) Kafka topics (third-party) Kafka partitions (third-party) Zookeeper quorum (third-party)","title":"S3 Replication Components Definition"},{"location":"requirements/s3-replication-monitoring/#storage-administrator-user-stories","text":"As a Storage Administrator I want a Monitoring Dashboard for S3 replication components So that I can identify configuration, performance, or network issues","title":"Storage Administrator User Stories"},{"location":"requirements/s3-replication-monitoring/#global-acceptance-criteria","text":"","title":"Global Acceptance Criteria"},{"location":"requirements/s3-replication-monitoring/#api","text":"S3 replication components metrics must be exposed through HTTP endpoint following Prometheus standards","title":"API"},{"location":"requirements/s3-replication-monitoring/#ui","text":"Third-party components dashboard must have an open-source monitoring dashboard S3 monitoring graphs should not show S3 replication operations S3 replication service must have a global monitoring dashboard containing: Current number of healthy vs unhealthy S3 replication components Number of objects replicated in the destinations over time Number of bytes replicated in the destinations over time Number of objects in the replication backlog over time Number of bytes in the replication backlog over time","title":"UI"},{"location":"requirements/s3-replication-monitoring/#alerting","text":"The product must allow to seed default alerting rules related to replication metrics.","title":"Alerting"},{"location":"requirements/s3-replication-monitoring/#deployment","text":"Replication monitoring components and monitoring dashboards must be deployed automatically in the product.","title":"Deployment"},{"location":"requirements/s3-replication-monitoring/#documentation","text":"Product documentation must describe: Network flows required to monitor S3 replication components (source, port destination, and protocol used) S3 replication monitoring configuration steps S3 replication components metrics S3 replication monitoring dashboards Alerting definition","title":"Documentation"},{"location":"requirements/s3-replication-monitoring/#queue-populator-acceptance-criteria","text":"","title":"Queue Populator Acceptance Criteria"},{"location":"requirements/s3-replication-monitoring/#queue-populator-monitoring-api","text":"The Queue Populator must expose the following metrics: State (running, initializing, or failed) Raft session metadata journal read offset Raft session metadata journal size Number of metadata journal entries processed since startup Number of Kafka messages produced on the topic since startup The Queue Populator metrics must be filterable by: Raft session ID Host name Docker container name","title":"Queue Populator Monitoring API"},{"location":"requirements/s3-replication-monitoring/#queue-populator-monitoring-ui","text":"The Queue Populator must have the following monitoring graphs: Number of Queue Processor State Metadata journal lag Metadata journal entry processed Messaged queued","title":"Queue Populator Monitoring UI"},{"location":"requirements/s3-replication-monitoring/#queue-processor-acceptance-criteria","text":"","title":"Queue Processor Acceptance Criteria"},{"location":"requirements/s3-replication-monitoring/#queue-processor-monitoring-api","text":"The Queue Processor must expose the following metrics: State (running, idle, initializing, or failed) Kafka consumer offset in the Kafka partitions Offset of the last message produced in the Kafka partitions Number of messages consumed in the Kafka partitions since startup Operations generated to retrieve (GET) objects from the source since startup: Number of operation Status Average latency Total bandwidth consumed Operations generated to write (PUT) object data and metadata in the destination since startup: Number of operation Status Average latency Total bandwidth consumed Queue Processor metrics must be filterable by: Kafka partition ID Host name Docker container name","title":"Queue Processor Monitoring API"},{"location":"requirements/s3-replication-monitoring/#queue-processor-monitoring-ui","text":"The Queue Processor must have the following monitoring graphs: Number of Queue Processor State Partition lag Operations generated to retrieve (GET) objects on the source Number of operation Operation status (success, retry, failed) Average latency Total bandwidth consumed Operations generated to write (PUT) object data and metadata in the destination Number of operation Operation status (success, retry, failed) Average latency Total bandwidth consumed","title":"Queue Processor Monitoring UI"},{"location":"requirements/s3-replication-monitoring/#status-processor-acceptance-criteria","text":"","title":"Status Processor Acceptance Criteria"},{"location":"requirements/s3-replication-monitoring/#status-processor-monitoring-api","text":"The Status Processor must exposed the following metrics: State (running, idle, initializing, or failed) Kafka consumer offset in the Kafka partitions Operations generated to update objects metadata in the source since startup Number of operation Status Average latency Total bandwidth consumed The Status Processor metrics must be filtrable by: Kafka partition ID Host Name Docker container name","title":"Status Processor Monitoring API"},{"location":"requirements/s3-replication-monitoring/#status-processor-monitoring-ui","text":"The Status Processor must have the following monitoring graphs: Number of Status Processor State Partitions lag Operations generated to update objects metadata in the source Number of operation Operation status (success, retry, failed) Average latency Total bandwidth consumed","title":"Status Processor Monitoring UI"},{"location":"requirements/utapi-metrics-protection/","text":"UTAPI metrics protection Context UTAPI v1 And v2 consumption metrics can be retrieved using root user access keys or IAM users access keys. IAM user can have restricted access to specific metrics using IAM policies mechanism. However, root users have unlimited access to consumption metrics. For instance, accountA root user can access accountB metrics. Service provider customers are complaining about the current implementation. They would like metrics to be protected at the account level so that they can expose UTAPI to their customers. In the meantime, service providers have used this behavior to implement their billing system. So, if we change the current behavior, we need to provide service providers another way to bill their customers. Requirements Billing Story As a Storage Administrator I want to be able to list all storage accounts consumption metrics In order to bill storage account owners Acceptance criteria A single set of keys must allow retrieving all storage accounts consumption metrics Credentials must not allow to create, update or delete storage accounts, IAM, S3, or STS resources (e.g: buckets, objects, users, policies, roles\u2026) Protection Story As a Storage Account Owner(a.k.a root user), I want to prevent my IAM Account consumption metrics to be accessed by another Storage Account Owner In order to keep account consumption metrics private Acceptance criteria: Account A can't access Account B consumption metrics using root or non-root user credentials","title":"UTAPI metrics protection"},{"location":"requirements/utapi-metrics-protection/#utapi-metrics-protection","text":"","title":"UTAPI metrics protection"},{"location":"requirements/utapi-metrics-protection/#context","text":"UTAPI v1 And v2 consumption metrics can be retrieved using root user access keys or IAM users access keys. IAM user can have restricted access to specific metrics using IAM policies mechanism. However, root users have unlimited access to consumption metrics. For instance, accountA root user can access accountB metrics. Service provider customers are complaining about the current implementation. They would like metrics to be protected at the account level so that they can expose UTAPI to their customers. In the meantime, service providers have used this behavior to implement their billing system. So, if we change the current behavior, we need to provide service providers another way to bill their customers.","title":"Context"},{"location":"requirements/utapi-metrics-protection/#requirements","text":"","title":"Requirements"},{"location":"requirements/utapi-metrics-protection/#billing-story","text":"As a Storage Administrator I want to be able to list all storage accounts consumption metrics In order to bill storage account owners Acceptance criteria A single set of keys must allow retrieving all storage accounts consumption metrics Credentials must not allow to create, update or delete storage accounts, IAM, S3, or STS resources (e.g: buckets, objects, users, policies, roles\u2026)","title":"Billing Story"},{"location":"requirements/utapi-metrics-protection/#protection-story","text":"As a Storage Account Owner(a.k.a root user), I want to prevent my IAM Account consumption metrics to be accessed by another Storage Account Owner In order to keep account consumption metrics private Acceptance criteria: Account A can't access Account B consumption metrics using root or non-root user credentials","title":"Protection Story"}]}